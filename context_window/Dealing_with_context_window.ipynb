{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Strategies to tackle context window limitations\n",
        "Models always have a context window i.e. the length of words, actually **tokens**, it can process at a given time. This is an issue when we want to process any text that doesn't fit in this context window. We can break the text into chunks that can fit. In this notebook we demonstrate a number of techniques to tackle this issue."
      ],
      "metadata": {
        "id": "J35hKZIjRgUv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oX9O0gIl0sQI"
      },
      "outputs": [],
      "source": [
        "text = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixed size chunking\n",
        "\n",
        "The simplest approach is to make chunks that fit in your context window without worrying about where it cuts the text. This, as you can see in the example below, is not ideal. Some words get chopped and when the LLM will see these chunks separately, it won't be able to infer information correctly."
      ],
      "metadata": {
        "id": "6SHJvzVP0y6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 20\n",
        "[text[i: i + chunk_size] for i in range(0, len(text), chunk_size)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUKdalek0xeI",
        "outputId": "4644fcac-b7ba-4755-bcda-5f17a7852e87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The dominant sequenc',\n",
              " 'e transduction model',\n",
              " 's are based on compl',\n",
              " 'ex recurrent or conv',\n",
              " 'olutional neural net',\n",
              " 'works in an encoder-',\n",
              " 'decoder configuratio',\n",
              " 'n. The best performi',\n",
              " 'ng models also conne',\n",
              " 'ct the encoder and d',\n",
              " 'ecoder through an at',\n",
              " 'tention mechanism. W',\n",
              " 'e propose a new simp',\n",
              " 'le network architect',\n",
              " 'ure, the Transformer',\n",
              " ', based solely on at',\n",
              " 'tention mechanisms, ',\n",
              " 'dispensing with recu',\n",
              " 'rrence and convoluti',\n",
              " 'ons entirely.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting based on special characters\n",
        "\n",
        "We can improve on this and start splitting by special characters such as \".\" or \"\\n\". This keeps most semantic information in the same chunk. But of course, there can be cases where some information is lost in a previous chunk, sentence when we split by \".\"."
      ],
      "metadata": {
        "id": "jfehDVe606pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.split(\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVoP1JK308M9",
        "outputId": "eb302c15-a6f4-4402-925b-36250302502a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration',\n",
              " ' The best performing models also connect the encoder and decoder through an attention mechanism',\n",
              " ' We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overlap between chunks\n",
        "\n",
        "To try to not lose this semantic information too much, we can add some overlap between chunks. This way some information is trickled in from the previous chunk and some from the next. Imagine this as reading the last two lines of the last paragraph and the first two of the next alongside the current paragraph you are reading."
      ],
      "metadata": {
        "id": "usn4EwjU0-37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text.split(\".\")\n",
        "overlap = 15\n",
        "[sentences[i-1][-overlap:] + sentences[i] + sentences[i+1][:5]  for i in range(0, len(sentences)-1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ehupwNZ0_rB",
        "outputId": "b001a926-29a9-41e0-9e4e-5e1bbe8cbac7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration The ',\n",
              " 'r configuration The best performing models also connect the encoder and decoder through an attention mechanism We p',\n",
              " 'ntion mechanism We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings, Vectors, RAG\n",
        "\n",
        "If there are too many chunks to process all of them every time a query is made, a RAG, Retrieval Augmented Generation, approach can be used. This is usually used with a vector database to do a similarity search to find relevant chunks before querying the LLM."
      ],
      "metadata": {
        "id": "34-D1WID1E4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers chromadb"
      ],
      "metadata": {
        "id": "HjFKpE3p1MRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "S0wcB0ZN2Wjh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use an embedding model to find suitable vectors to represent our vocabulary whether it is words or sentences. These vectors are then stored in a vector database for retrieval later."
      ],
      "metadata": {
        "id": "8MQ1mgVp3DDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "sentences = text.split(\".\")\n",
        "\n",
        "text_embeddings = model.encode(sentences)\n",
        "print(text_embeddings.shape)"
      ],
      "metadata": {
        "id": "KG9jzPrJ1N-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChromaDB is an Open Source Vector Database that we can use for our RAG application to query before the request is sent to the LLM. ChromaDB as default uses the same Sentence Embedding model we used above, \"all-MiniLM-L6-v2\"."
      ],
      "metadata": {
        "id": "ou2osWA9-8FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection(name=\"MySentenceStore\")\n",
        "collection.add(documents=sentences, ids=[str(id) for id in range(0, len(sentences))])"
      ],
      "metadata": {
        "id": "Hg9ApDhO2BZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we send our query to the LLM, we find a relevant chunk from our vector database. In this example, it will give us the sentence most relevant to our question."
      ],
      "metadata": {
        "id": "im0r4ltVgR1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_results = collection.query(query_texts=[\"What do the best performing models do?\"], n_results=1)"
      ],
      "metadata": {
        "id": "D0zUNATh6Uh9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_results[\"documents\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9fISosS7RdL",
        "outputId": "90a96c9b-bdab-4617-8a3e-69759fda8d34"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[' The best performing models also connect the encoder and decoder through an attention mechanism']]\n"
          ]
        }
      ]
    }
  ]
}