{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94696b6",
   "metadata": {},
   "source": [
    "---\n",
    "author: Martiño Ríos García\n",
    "date: 2024-05-17\n",
    "title: 4 | Choosing the learning paradigm\n",
    "keep-ipynb: True\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49bea5",
   "metadata": {},
   "source": [
    "The objective of this book is to illustrate with a practical example how to decide which learning paradigm is better for each application. To illustrate the process, some information about chemical reactions will be extracted from paragraphs of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496df9f4",
   "metadata": {},
   "source": [
    "## First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebabaf",
   "metadata": {},
   "source": [
    "Choosing the learning paradigm should begin trying some models with a general pre-training. For this practical case, the first model to test is the very recent Llama-3 8B model with a zero and one-shot prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391713cc",
   "metadata": {},
   "source": [
    "We will start by importing all the packages needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38bd759c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -qqq torch transformers datasets evaluate accelerate peft trl bitsandbytes langchain python-dotenv litellm numpy matplotlib --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544ff2bd",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Hide the Wandb login\n",
    "os.environ[\"WANDB_SILENT\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1777e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    ")\n",
    "from trl import (\n",
    "    SFTTrainer,\n",
    "    DataCollatorForCompletionOnlyLM,\n",
    ")\n",
    "from evaluate import load\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from litellm.caching import Cache\n",
    "from statistics import mean\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0dcf5f",
   "metadata": {},
   "source": [
    "To continue we will allow for the caching of `LiteLLM`. Additionally, we will import all environment variables. \n",
    "\n",
    "::: {.column-margin}\n",
    "Note that using the environment variables is the safest way of keeping personal API keys secret.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94595c41",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "litellm.cache = Cache()\n",
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e82b75",
   "metadata": {},
   "source": [
    "## First model and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31412f42",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "It is important to include the *HF_token* in the *.env* file. By the time this notebook was created, the model that we are going to fine-tune (Llama3-8B) is only available after request for access.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b1daf",
   "metadata": {},
   "source": [
    "As starting model, we will try the Llama-3 8B model. We will call this model through the *Groq API*, which allows performing fast inference with several open-source models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3fa61",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "Groq is a provider of some of the most popular open-source models such as Llama or Mixtral models with a high inference speed. To use the Groq API, it also needed to add the *GROQ_API_KEY* to the *.env* file.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b3dae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"groq/llama3-8b-8192\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e61119",
   "metadata": {},
   "source": [
    "The dataset used in this tutorial is the one used in Ai et al.'s [@Ai_2024] recent work, which contains data about chemical reactions. The dataset contains 100K reaction procedure—ORD JSON pairs. \n",
    "\n",
    "::: {.column-margin}\n",
    "**ORD** stands for *Open Reaction Database* schema, a comprehensive data structure specially designed to describe all the elements involved in chemical reactions.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8df5d",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-tip title=\"Download data\" collapse=\"true\"}\n",
    "\n",
    "To obtain the data, the best way is to install the *GitHub* repository of the Ai et al. work and take from there. To do so, run the following commands: \n",
    "\n",
    "```\n",
    "!git clone https://github.com/qai222/LLM_organic_synthesis.git\n",
    "!cp LLM_organic_synthesis/workplace_data/datasets/USPTO-n100k-t2048_exp1.7z .\n",
    "!7za x USPTO-n100k-t2048_exp1.7z\n",
    "!cp USPTO-n100k-t2048_exp1/*.json .\n",
    "!rm -rf USPTO-n100k-t2048_exp1/ USPTO-n100k-t2048_exp1.7z LLM_organic_synthesis\n",
    "```\n",
    "This will leave four *.json* files in your current directory that contain all the data used here.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5711cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'instruction'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds_path = \"test.json\"\n",
    "test_dataset = load_dataset(\"json\", data_files=test_ds_path, split=\"train\")\n",
    "test_dataset = test_dataset.shuffle(seed=42).select(range(100))\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc97d1f-827d-4b09-9149-63c42083a78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '{\"inputs\": {\"m1\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"4-nitrobenzyl (1R,3R,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-oxo-1-carbapenam-3-carboxylate\"}], \"amount\": {\"mass\": {\"value\": 743.0, \"units\": \"MILLIGRAM\"}}, \"reaction_role\": \"REACTANT\"}]}, \"m2\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"2-(tri-n-butylstannyl)-7-trifluoromethylthioimidazo[5,1-b]thiazole\"}], \"amount\": {\"mass\": {\"value\": 1.06, \"units\": \"GRAM\"}}, \"reaction_role\": \"REACTANT\"}]}}, \"conditions\": {\"conditions_are_dynamic\": true}, \"outcomes\": [{\"products\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"4-nitrobenzyl (1S,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-(7-trifluoromethylthioimidazo[5,1-b]thiazol-2-yl)-1-carbapen-2-em-3-carboxylate\"}], \"measurements\": [{\"type\": \"AMOUNT\", \"details\": \"MASS\", \"amount\": {\"mass\": {\"value\": 172.0, \"units\": \"MILLIGRAM\"}}}], \"reaction_role\": \"PRODUCT\"}]}]}',\n",
       " 'instruction': 'Below is a description of an organic reaction. Extract information from it to an ORD JSON record.\\n\\n### Procedure:\\nThe procedure of Example 1a) was repeated, except that 743 mg of 4-nitrobenzyl (1R,3R,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-oxo-1-carbapenam-3-carboxylate and 1.06 g 2-(tri-n-butylstannyl)-7-trifluoromethylthioimidazo[5,1-b]thiazole were used as the starting compounds. Thus, 172 mg of 4-nitrobenzyl (1S,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-(7-trifluoromethylthioimidazo[5,1-b]thiazol-2-yl)-1-carbapen-2-em-3-carboxylate was prepared.\\n\\n### ORD JSON:\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1c2be-7681-4087-bc31-8e335d9e9dd9",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "Note that the output comes in JSON format. For this simple example we are not going to constrain the output to JSON format. However, to ensure good results this constraining must be done. Deep explanation and examples about the options and how to constrain the output are provided in the chapter and its notebook \"*Constrained decoding and enforcing valid outputs*\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf630b",
   "metadata": {},
   "source": [
    "This dataset is very big. Therefore, we will only take 100 samples from the test set used in the article mentioned above for our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6accdc51",
   "metadata": {},
   "source": [
    "## Prompt and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6bc953",
   "metadata": {},
   "source": [
    "We define a simple prompt template. The prompt contains a simple *system* part (named as *PREFIX*) where the role and task of the model is defined as well as the example used only for the 1-shot prompt. \n",
    "Additionally, the prompt has a *user* prompt where the reaction instruction will be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d274b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"You are a helpful scientific assistant. Your task is to extract information about organic reactions. {shot}\"\"\"\n",
    "SUFFIX = \"\"\"\\n\\n{sample}\\n\\n\"\"\"\n",
    "SHOT = \"\"\"\n",
    "One example is provided to you to show how to perform the task:\n",
    "\n",
    "### Procedure:\\nA suspension of 8 g of the product of Example 7 and 0.4 g of DABCO in 90 ml of xylenes were heated under N2 at 130\\u00b0-135\\u00b0 C. while 1.8 ml of phosgene was added portionwise at a rate to maintain a reflux temperature of about 130\\u00b0-135\\u00b0 C. The mixture was refluxed an additional two hours, cooled under N2 to room temperature, filtered, and the filtrate was concentrated in vacuo to yield 6.9 g of the subject compound as a crude oil.\\n\\n\n",
    "### ORD JSON:\\n{\\\"inputs\\\": {\\\"m1_m2_m4\\\": {\\\"components\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"product\\\"}], \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 8.0, \\\"units\\\": \\\"GRAM\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}, {\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"DABCO\\\"}], \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 0.4, \\\"units\\\": \\\"GRAM\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}, {\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"xylenes\\\"}], \\\"amount\\\": {\\\"volume\\\": {\\\"value\\\": 90.0, \\\"units\\\": \\\"MILLILITER\\\"}}, \\\"reaction_role\\\": \\\"SOLVENT\\\"}]}, \\\"m3\\\": {\\\"components\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"phosgene\\\"}], \\\"amount\\\": {\\\"volume\\\": {\\\"value\\\": 1.8, \\\"units\\\": \\\"MILLILITER\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}]}}, \\\"conditions\\\": {\\\"temperature\\\": {\\\"control\\\": {\\\"type\\\": \\\"AMBIENT\\\"}}, \\\"conditions_are_dynamic\\\": true}, \\\"workups\\\": [{\\\"type\\\": \\\"ADDITION\\\", \\\"details\\\": \\\"was added portionwise at a rate\\\"}, {\\\"type\\\": \\\"TEMPERATURE\\\", \\\"details\\\": \\\"to maintain a reflux temperature of about 130\\\\u00b0-135\\\\u00b0 C\\\"}, {\\\"type\\\": \\\"TEMPERATURE\\\", \\\"details\\\": \\\"The mixture was refluxed an additional two hours\\\", \\\"duration\\\": {\\\"value\\\": 2.0, \\\"units\\\": \\\"HOUR\\\"}}, {\\\"type\\\": \\\"FILTRATION\\\", \\\"details\\\": \\\"filtered\\\"}, {\\\"type\\\": \\\"CONCENTRATION\\\", \\\"details\\\": \\\"the filtrate was concentrated in vacuo\\\"}], \\\"outcomes\\\": [{\\\"products\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"subject compound\\\"}], \\\"measurements\\\": [{\\\"type\\\": \\\"AMOUNT\\\", \\\"details\\\": \\\"MASS\\\", \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 6.9, \\\"units\\\": \\\"GRAM\\\"}}}], \\\"reaction_role\\\": \\\"PRODUCT\\\"}]}]}\n",
    "\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edaac7-3bd9-4a10-90a0-0a979ea3bf2d",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "**PREFIX** is supposed to be content of the system prompt.\n",
    "**SUFFIX** is the user prompt.\n",
    "**SHOT** is the 1-shot prompt that will be added to the system prompt when used.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece9187",
   "metadata": {},
   "source": [
    "To continue, we loop all over the dataset two times, one for each type of prompt (zero and one-shot). For each dataset sample, we format the prompt to include the procedure-output schema pairs using the template defined in the previous cell. In addition, we also predict using the model and store those predictions for future evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704263f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = ['0-shot', '1-shot']\n",
    "results_llama = {}\n",
    "\n",
    "# Start by looping over the shots\n",
    "for s in shots:\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "# Loop over all the samples of the dataset\n",
    "    for t in test_dataset:\n",
    "        instruction = t[\"instruction\"]\n",
    "        output = t[\"output\"]\n",
    "        # Format the prompt following OpenAI's prompting guidelines\n",
    "        if s == '0-shot':\n",
    "            shot = \"\"\n",
    "        else:\n",
    "            shot = SHOT\n",
    "        system = PREFIX.format(shot=shot)\n",
    "        user = SUFFIX.format(sample=instruction)\n",
    "        prompt = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ]\n",
    "        # Do the completion using Groq API through LiteLLM\n",
    "        pred = (\n",
    "            completion(\n",
    "                model=base_model,\n",
    "                messages=prompt,\n",
    "                caching=True,\n",
    "                temperature=0,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        # Save the predictions and the references for later evaluation\n",
    "        references.append(output)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    results_llama[s] = {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198f48f-8267-46b8-9d5f-2177fadcde72",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "The beauty of *LiteLLM*is that it allows to do completions for a bunch of models by different providers using for all the *OpenAI* prompt completions schema:\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "\n",
    "This format of completions is probably the most extended one, and is really intuitive.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67caa2b",
   "metadata": {},
   "source": [
    "After generating the predictions, it's essential to evaluate them. We will initially use BERT Score for a simple evaluation, as it provides precision, recall, and F$_1$-scores based on similarity measures. However, for a complex schema like the one we are predicting, a more robust evaluation method should be utilized. (REF EVALUATION CHAPTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138e781",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "shots = ['0-shot', '1-shot']\n",
    "\n",
    "# Start by looping over the shots\n",
    "for s in shots:\n",
    "    predictions = results_llama[s][\"predictions\"]\n",
    "    references = results_llama[s][\"references\"]\n",
    "\n",
    "    results_ = bertscore.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        model_type=\"distilbert-base-uncased\",\n",
    "    )\n",
    "\n",
    "    results_llama[s].update(\n",
    "        {\n",
    "            \"precision\": mean(results_[\"precision\"]),\n",
    "            \"recall\": mean(results_[\"recall\"]),\n",
    "            \"f1_scores\": mean(results_[\"f1\"]),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbe43b-621c-4eb5-b826-94406a08b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in shots:\n",
    "    print(f\"Results for the {s} prompt\")\n",
    "    print(f\"\\tPrecision: {results_llama[s]['precision']}\")\n",
    "    print(f\"\\tRecall: {results_llama[s]['recall']}\")\n",
    "    print(f\"\\tF1-Score: {results_llama[s]['f1_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b348dea-10c5-41ee-8f4e-c8c8a8cc6985",
   "metadata": {},
   "source": [
    "The results are really good, specially with the 1-shot prompt. However we are going to try next another different model, a closed-source in order to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3a5ff",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Remember to safe the results for future evaluation!\n",
    "with open('Llama_results.json', 'w') as f:\n",
    "   json.dump(results_llama, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6ddbe",
   "metadata": {},
   "source": [
    "## Another model, closed-source this time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86603155",
   "metadata": {},
   "source": [
    "The second model we will use is the newer *OpenAI* model, the *GPT-4o* model. This allows us to compare open- and closed-source models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cda5ff",
   "metadata": {},
   "source": [
    "The procedure and code are exactly the same as for the previous case; the only difference is to define a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f62cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cc695",
   "metadata": {},
   "source": [
    "And we do the completions using both prompts for all the test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad7850-dc12-4cfd-8c71-338c95895f06",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "*OpenAI* models are also supported by the *LiteLLM* package.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b9954-2646-4939-99b5-f5bdfd6c4a36",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "results_openai = {}\n",
    "shots = [\"0-shot\", \"1-shot\"]\n",
    "\n",
    "# Start by looping over the shots\n",
    "for s in shots:\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "# Loop over all the samples of the dataset\n",
    "    for t in test_dataset:\n",
    "        instruction = t[\"instruction\"]\n",
    "        output = t[\"output\"]\n",
    "        # Format the prompt following OpenAI's prompting guidelines\n",
    "        if s == \"0-shot\":\n",
    "            shot = \"\"\n",
    "        else:\n",
    "            shot = SHOT\n",
    "        system = PREFIX.format(shot=shot)\n",
    "        user = SUFFIX.format(sample=instruction)\n",
    "        prompt = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ]\n",
    "        # Do the completion using Groq API through LiteLLM\n",
    "        pred = (\n",
    "            completion(\n",
    "                model=base_model,\n",
    "                messages=prompt,\n",
    "                caching=True,\n",
    "                temperature=0,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        # Remove some residual stuff in the json output by the model.\n",
    "        if \"```json\" in pred:\n",
    "            pred = pred.replace(\"```json\\n\", \"\")\n",
    "            pred = pred.replace(\"```\", \"\")\n",
    "            \n",
    "        # Save the predictions and the references for later evaluation\n",
    "        references.append(output)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    results_openai[s] = {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15351952",
   "metadata": {},
   "source": [
    "Finally, we evaluate again using *BERT-score*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a9070",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "for s in shots:\n",
    "    predictions = results_openai[s]['predictions']\n",
    "    references = results_openai[s]['references']\n",
    "\n",
    "    results_ = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "    results_openai[s].update({\n",
    "        'precision': mean(results_['precision']),\n",
    "        'recall': mean(results_['recall']),\n",
    "        'f1_scores': mean(results_['f1']),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc80d6-1f72-4ca7-8ab8-772a4c43437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in shots:\n",
    "    print(f\"Results for the {s} prompt\")\n",
    "    print(f\"\\tPrecision: {results_llama[s]['precision']}\")\n",
    "    print(f\"\\tRecall: {results_llama[s]['recall']}\")\n",
    "    print(f\"\\tF1-Score: {results_llama[s]['f1_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007a3d8-ba1b-4424-a41f-eba3ec1e07e2",
   "metadata": {},
   "source": [
    "The results with this *GPT-4o* model are really good, improving a bit the ones obtained with *Llama-3 8B* base model. However, we are going to try to further improve this results by fine-tuning the *Llama-3 8B* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca60379-12e0-416b-a03a-cd8e89cb9a21",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Remember to safe the results for future evaluation!\n",
    "with open('OpenAI_results.json', 'w') as f:\n",
    "   json.dump(results_openai, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407d832",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51e704",
   "metadata": {},
   "source": [
    "As the final step, we will fine-tune the *Llama-3 8B* using data similar to the one we used above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8af77",
   "metadata": {},
   "source": [
    "We will use packages built by *HuggingFace* to do the fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f4248",
   "metadata": {},
   "source": [
    "First, we define the base model we will use and the path of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4470537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51072b95",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "This will make it easy to track training progress using [Weights and Biases](https://wandb.ai/), allowing quick access to the loss curve and other important training parameters.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b3d824",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c410452-558b-4d0d-82c3-984cb6cd5af8",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "In order the `wandb.login()`works, the `.env` imported previously must contain the personal *WANDB_API_KEY*.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330aff3c",
   "metadata": {},
   "source": [
    "The next step is to load the dataset for the fine-tuning. For that, as for the testing from previous models, we will use the dataset used by Ai et al., but for this case, we will use their **train** dataset. Since this is a quick demonstration, we will only take 5000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37b7756e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'instruction'],\n",
       "        num_rows: 4500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['output', 'instruction'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files='train.json', split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(5000)) # Only use 5000 samples for quick demo\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42) # We define 90-10 % training-evaluation splits.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731608d",
   "metadata": {},
   "source": [
    "Then we define the method to fine-tune and all the parameters. For this fine-tuning we will be using the popular QLoRA method. QLoRA [@dettmers2023qlora] is an efficient finetuning approach that reduces memory usage during finetuning, while preserving full finetuning task performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee542f-6eff-40d2-9c75-e49c6ca49794",
   "metadata": {},
   "source": [
    "::: {.callout-tip title=\"Notes about QLoRA configuration\" collapse=\"true\"}\n",
    "- **load_in_4bit=True**: loads the model using the 4-bit quatization.\n",
    "- **bnb_4bit_quant_type=\"nf4\"**: quatizacies following the nf4 method.[@dettmers20228bit]\n",
    "- **bnb_4bit_use_double_quant=True**: activate nested quantization for 4-bit base models.\n",
    "- **bnb_4bit_compute_dtype=torch.bfloat16**: Compute dtype for 4-bit base models.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a1cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # fp4 or nf4\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9d59b-6466-40dd-9a79-4c0fe2ba7da4",
   "metadata": {},
   "source": [
    "::: {.callout-tip title=\"Notes about LoRA configuration\" collapse=\"true\"}\n",
    "- **r**: The rank of the updated matrices, expressed as integer. Lower rank results in smaller update matrices with fewer trainable parameters. It means that the adaptor that is build in top of the model to improved will be made by matrices of rank 32.\n",
    "- **lora_alpha**: LoRA scaling factor. It changes how the adaptation layer's weights affect the base model's.\n",
    "- **lora_dropout**: Dropout is a regularization technique where a proportion of neurons (or parameters) are randomly “dropped out” or turned off during training to prevent overfitting.\n",
    "- **bias**: Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'.\n",
    "- **task_type**: Task to perform, \"Causal LM\": Causal language modeling.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "395f3937-597e-474b-8aeb-39fcffab1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64, \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2d0e6",
   "metadata": {},
   "source": [
    "Before training, we define the tokenizer and the model for fine-tuning, set the training arguments, and initialize the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c24ce1",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8b129becc4492dbca540ba02594615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model) # Define the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Where the \"pad_token\" is placed\n",
    "\n",
    "# Model config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model, # Model that we are going to fine-tune\n",
    "    quantization_config=bnb_config, # QLoRA config defined above\n",
    "    device_map=\"auto\", # Where the model is trained, set device_map=\"auto\" loads a model onto available GPUs first.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96452c65-53ab-4fe7-bc63-e68340840e6a",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "The *pad_token* is a special token used to make arrays of tokens the same size for batching purpose. The typical is to use the \"eos_token\" which is a special token representing the end of a sentence.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b5c68-06b8-4282-822a-3522bea24497",
   "metadata": {},
   "source": [
    "::: {.callout-tip title=\"Notes about the training arguments\" collapse=\"true\"}\n",
    "- **learning_rate**: the learning rate is a hyper-parameter that sets how the training algorithm updates the values of the weights.\n",
    "- **Batch size**: it is the number of samples used in one forward and backward pass through the network. Ideally we would like to increase this number so the fine-tuning would last less. The problem is that for higher batch number, more GPU memory is needed. For example, for training the model used in this demostration using the exact same configuration but with a default token length (1024 tokens), with 40GB vRAM GPU, the maximum batch number is 2. Using 80GB of vRAM GPU the batch size can be increased to 4.\n",
    "    - **per_device_train_batch_size**: batch size for the training.\n",
    "    - **per_device_eval_batch_size**: batch size for the evaluation.\n",
    "- **gradient_accumulation_steps**: Number of accumulated gradients over each batch.\n",
    "- **optim**: optimizer used. The main role of the optimizer is to minimize the loss function. The **paged_adamw_32bit** is the well known **AdamW** optimizer. **AdamW** optimization is a stochastic gradient descent method.[@loshchilov2019decoupled]\n",
    "- **num_train_epochs**: number of times that the model goes through each sample during the training. Larger number might lead to best training results or to overfitting. Lower number might give a model that do not work as expected at all.\n",
    "- **fp16** and **bf16**: these parameters helps achieving mixed precision training which is a technique that aims to optimize the computational efficiency of training models by utilizing lower-precision numerical formats for certain variables. However, choosing of both parameters depends of the architecture of the GPUs that are being used during the training.\n",
    "- **logging_steps**: when the logging is done.\n",
    "- **evaluation_strategy**: the evaluation strategy to adopt during training. The most used is 'steps' meaning that the evaluation is done after a certain number of training steps.\n",
    "- **eval_steps**: define in which steps the evaluation is done.\n",
    "- **max_grad_norm**: maximum gradient norm (for gradient clipping). Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and using the clipped gradients to update the weights.\n",
    "- **warmup_steps**: number of steps used for a linear warmup from 0 to learning_rate. The warmup helps to stabilize the optimization process and prevent divergence.\n",
    "- **warmup_ratio**: ratio of total training steps used for the linear warmup.\n",
    "- **group_by_length**: whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding.\n",
    "- **lr_scheduler_type**: describes the decayment of the learning rate during the training.\n",
    "- **output_dir**: directory to safe the report of the training process.\n",
    "- **save_strategy**: what we want to safe during the training. Set it to \"no\" to only safe the final model.\n",
    "\n",
    "![](cosine.png){#fig-cosine} Shape of the \"cosine\" lr_scheduler_type option\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10da3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different hyperparameters and arguments for the fine-tuning\n",
    "training_arguments = TrainingArguments(\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=10,\n",
    "    fp16=False,\n",
    "    bf16=True, #bf16 to True with an A100, False otherwise\n",
    "    logging_steps=1, # Logging is done every step.\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=0.05,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_steps=10, \n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    output_dir=\"./results/\", \n",
    "    save_strategy='no', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f750cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \" ### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253da04a-c0cb-40bf-8cd1-799d894bfa7d",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "**Data collators** are objects that will form a batch by using a list of dataset elements as input. There is one data collator for each task, here we use the one for completion-only.\n",
    ":::\n",
    "::: {.column-margin}\n",
    "The **completion-only training** instead of training the model on the whole input (prompt + answer) make the training more efficient by training only the model on completion.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21d42807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd200ffc-515c-441b-b41c-47187b25f469",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "The **formatting function** is intended for those cases where the prompt is divided in more than one feature of the dataset. Using the a formatting functions allow us to join them in the best way.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49f7e685",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model, # Model to fine-tune\n",
    "    max_seq_length=2048, # Max number of tokens of the completion\n",
    "    args=training_arguments, # Training arguments to use \n",
    "    train_dataset=dataset[\"train\"], # Set of the dataset used for the training\n",
    "    eval_dataset=dataset[\"test\"], # Set of the dataset used for the evaluations\n",
    "    peft_config=peft_config, # Configuration and PEFT method to use\n",
    "    tokenizer=tokenizer, # Tokenizer used \n",
    "    packing=False,\n",
    "    formatting_func=formatting_prompts_func, # Prompt formatting function\n",
    "    data_collator=collator, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77200618-299f-4c62-a447-2d88ff0bc33a",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "When *packing* is set to True means during the training, multiple short examples are pack in the same input sequence to increase training efficiency. However, when using a collator it must be False.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73f08e",
   "metadata": {},
   "source": [
    "And finally when everything is ready we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c3e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5367' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5367/11250 2:46:41 < 3:02:47, 0.54 it/s, Epoch 4.77/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.592538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1126</td>\n",
       "      <td>0.404200</td>\n",
       "      <td>0.572942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1689</td>\n",
       "      <td>0.509100</td>\n",
       "      <td>0.566474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2252</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.563503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2815</td>\n",
       "      <td>0.553200</td>\n",
       "      <td>0.562221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3378</td>\n",
       "      <td>0.472700</td>\n",
       "      <td>0.561783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3941</td>\n",
       "      <td>0.495300</td>\n",
       "      <td>0.561582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4504</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>0.561479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5067</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.561480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca71028",
   "metadata": {},
   "source": [
    "We can consult the loss curves from WandB to see if the training performs well and if there is no overfitting or other strange behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a4f3e",
   "metadata": {},
   "source": [
    "::: {#fig-loss_curves layout-ncol=2}\n",
    "\n",
    "![Training-loss curve](train_loss.png){#fig-eloss}\n",
    "\n",
    "![Evaluation-loss curve](val_loss.png){#fig-tloss}\n",
    "\n",
    "Loss curves from the fine-tuning that were reported to *WandB*. Both curves looks such as they should be when the training goes fine.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c6e16",
   "metadata": {},
   "source": [
    "To evaluate the fine-tuned model and do inference, the easiest way is to use the trained model directly. To do that we have to define a *pipeline* for text-generation and then do the inference using that *pipeline* and evaluate in a similar way as for the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa71e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline that will do the inference\n",
    "sft_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    temperature=0.01,\n",
    "    model=trainer.model, # We do the inference with the trained model.\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4064b-e67e-448d-95d5-f5d54f6a742c",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "The temperature defines the freedom degrees that are allowed to the model. For data extraction the best value is 0 but HuggingFace do not allow a round 0, so we have to set a number very close to 0.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the inference for all the samples of the test set using 0 and 1-shot prompts.\n",
    "results_sft = {}\n",
    "shots = ['0-shot', '1-shot']\n",
    "\n",
    "# Start by looping over the shots\n",
    "count = 0\n",
    "for s in shots:\n",
    "    count+=1\n",
    "    print(count)\n",
    "    references = []\n",
    "    predictions_sft = []\n",
    "    prompts = []\n",
    "    # Loop over all the samples of the dataset\n",
    "    for t in test_dataset:\n",
    "        instruction = t['instruction']\n",
    "        output = t['output']\n",
    "        if s == '0-shot':\n",
    "            shot = ''\n",
    "        else:\n",
    "            shot = SHOT\n",
    "        # Format the prompt\n",
    "        system = PREFIX.format(shot=shot)\n",
    "        user = SUFFIX.format(sample=instruction)\n",
    "        prompt = system + user\n",
    "        references.append(output)\n",
    "        \n",
    "        # Do the completion using the pipeline defined above\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = sft_pipe(prompt)\n",
    "        # Clean the output\n",
    "        predictions_sft.append(pred[0]['generated_text'].replace(prompt, ''))\n",
    "    \n",
    "    results_sft [s] = {\n",
    "        \"predictions\": predictions_sft,\n",
    "        \"references\": references,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b773ace-9d9b-44bb-8f61-96d908132ef4",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "Note that for this particular case we do not use *LiteLLM*. This is because this package do not yet support Completion only task with *HuggingFace* models. Thus the prompt is not written following OpenAI completions guide.\n",
    ":::\n",
    "::: {.column-margin}\n",
    "For this case, note that in the completion we replace the prompt by nothing. This is done because the model was instructed to always give the completions with the prompt at the beginning. Above, for the case where we used this same Llama model, this was not done because the API already provides the completion without the prompt.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7521d82-6601-4148-8c3c-33d125540ed3",
   "metadata": {},
   "source": [
    "Finally, we just need to calculate the metrics to evaluate this last model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in shots:\n",
    "    predictions_sft = results_sft[s][\"predictions\"]\n",
    "    references = results_sft[s][\"references\"]\n",
    "\n",
    "    results = bertscore.compute(predictions=predictions_sft, references=references, model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "    results_sft[s].update({\n",
    "        \"precision\": mean(results[\"precision\"]),\n",
    "        \"recall\": mean(results[\"recall\"]),\n",
    "        \"f1_scores\": mean(results[\"f1\"]),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043e1f1-07fc-4219-bf41-82b17619ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in shots:\n",
    "    print(f\"Results for the {s} prompt\")\n",
    "    print(f\"\\tPrecision: {results_sft[s]['precision']}\")\n",
    "    print(f\"\\tRecall: {results_sft[s]['recall']}\")\n",
    "    print(f\"\\tF1-Score: {results_sft[s]['f1_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f4717-7a96-44d5-ae4f-fb1aec103554",
   "metadata": {},
   "source": [
    "The results are very similar as for the previous cases. But being honest with ourselfs, there was not a lot of room for big improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077abfb",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "with open('sft_results.json', 'w') as f:\n",
    "   json.dump(results_sft, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e9060",
   "metadata": {},
   "source": [
    "## Visualization of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fb551",
   "metadata": {},
   "source": [
    "To study the results in a more graphical way, we can plot all the results in two bars plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6f0c7",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "models = ['llama', 'sft', 'openai']\n",
    "metrics = ['precision', 'recall', 'f1_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941275f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Organize the results for easy plotting\n",
    "\n",
    "results = {}\n",
    "\n",
    "results['llama_results'] = results_llama\n",
    "results['openai_results'] = results_openai\n",
    "results['sft_results'] = results_sft\n",
    "\n",
    "metrics_0_shot = []\n",
    "metrics_1_shot = []\n",
    "for model in models:\n",
    "    tmp_0 = []\n",
    "    tmp_1 = []\n",
    "    for metric in metrics:\n",
    "        tmp_0.append(results[model + '_results']['0-shot'][metric])\n",
    "        tmp_1.append(results[model + '_results']['1-shot'][metric])\n",
    "    metrics_0_shot.append(tmp_0)\n",
    "    metrics_1_shot.append(tmp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaeab06",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# set width of bar\n",
    "barWidth = 0.2\n",
    "fig = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "plt_models = [\"Llama3-8B\", \"Llama3-8B Fine-tuned\", \"GPT-4o\"]\n",
    "plt_metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
    "plt_data = {}\n",
    "for index, model in enumerate(plt_models):\n",
    "    plt_data[model] = metrics_0_shot[index]\n",
    "\n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(metrics_0_shot[0]))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(\n",
    "    br1,\n",
    "    metrics_0_shot[0],\n",
    "    color=\"b\",\n",
    "    width=barWidth,\n",
    "    edgecolor=\"black\",\n",
    "    label=plt_models[0],\n",
    ")\n",
    "plt.bar(\n",
    "    br2,\n",
    "    metrics_0_shot[1],\n",
    "    color=\"skyblue\",\n",
    "    width=barWidth,\n",
    "    edgecolor=\"black\",\n",
    "    label=plt_models[1],\n",
    ")\n",
    "plt.bar(\n",
    "    br3,\n",
    "    metrics_0_shot[2],\n",
    "    color=\"mediumseagreen\",\n",
    "    width=barWidth,\n",
    "    edgecolor=\"black\",\n",
    "    label=plt_models[2],\n",
    ")\n",
    "\n",
    "# Adding Xticks\n",
    "plt.xlabel(\"Metric\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"Results\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xticks(\n",
    "    [r + barWidth for r in range(len(metrics_0_shot[0]))], plt_metrics, fontsize=12\n",
    ")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Different metrics with 0 shot prompt for the models\", fontsize=20)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"bars0.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3cc0c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# set width of bar\n",
    "barWidth = 0.2\n",
    "fig = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "plt_models = [\"Llama3-8B\", \"Llama3-8B Fine-tuned\", \"GPT-4-turbo\"]\n",
    "plt_metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
    "plt_data = {}\n",
    "for index, model in enumerate(plt_models):\n",
    "    plt_data[model] = metrics_1_shot[index]\n",
    "\n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(metrics_1_shot[0]))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(\n",
    "    br1,\n",
    "    metrics_1_shot[0],\n",
    "    color=\"b\",\n",
    "    width=barWidth,\n",
    "    edgecolor=\"black\",\n",
    "    label=plt_models[0],\n",
    ")\n",
    "plt.bar(\n",
    "    br2,\n",
    "    metrics_1_shot[1],\n",
    "    color=\"skyblue\",\n",
    "    width=barWidth,\n",
    "    edgecolor=\"black\",\n",
    "    label=plt_models[1],\n",
    ")\n",
    "plt.bar(\n",
    "    br3,\n",
    "    metrics_1_shot[2],\n",
    "    color=\"mediumseagreen\",\n",
    "    width=barWidth,\n",
    "    edgecolor=\"black\",\n",
    "    label=plt_models[2],\n",
    ")\n",
    "\n",
    "# Adding Xticks\n",
    "plt.xlabel(\"Metric\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"Results\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xticks(\n",
    "    [r + barWidth for r in range(len(metrics_1_shot[0]))], plt_metrics, fontsize=12\n",
    ")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Different metrics with 1-shot prompt for the models\", fontsize=20)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"bars1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6dc4d0",
   "metadata": {},
   "source": [
    "The results for the three models are quite good, and there are no significant differences between them. This is partly due to the evaluation method used; however, employing a more robust evaluation will lead to more noticeable differences. For this reason, we recommend reviewing the Evaluations section and following the guidelines outlined there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ac978",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "::: {#refs}\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
