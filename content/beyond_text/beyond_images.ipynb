{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da701e21fecca2a0",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#  Beyond text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5e72798406fa1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP-LLMs tend to have problems with analysing and understanding complex structures such as tables, plots and images included in scientific articles. Since especially in chemistry and material science information about chemical components is included in these, one should think about different approaches for these structures. Therefore, vision language models (VLMs) since they can analyse images alongside text. There are several open and closed-source VLMs available e.g. [Vision models from OpenAI](https://platform.openai.com/docs/guides/vision), [Claude models](https://docs.anthropic.com/en/docs/vision) and [DeepSeek-VL](https://github.com/deepseek-ai/DeepSeek-VL). As an example, we show the extraction of images with [GPT4-o](https://platform.openai.com/docs/models/gpt-4o).\n",
    "\n",
    "First one has to convert the file into images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d590539057c1dc3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```{note}\n",
    "\n",
    "The used PDF file was obtained in the [Section 1](../obtaining_data/data_mining.ipynb).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b886d18ec7e86797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T10:42:11.479846Z",
     "start_time": "2024-06-03T10:42:10.275394Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import llmstructdata  # noqa: F401\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "file_path = \"../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf\"\n",
    "\n",
    "# converting the PDF files to images\n",
    "pdf_images = convert_from_path(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d2d2c714687fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that one should process the obtained images fo instance rotate pages with vertical text since many models have problems with this. Next one has to convert the pictures into machine-readable Base-64 format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b39a0896010c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T10:42:19.110545Z",
     "start_time": "2024-06-03T10:42:11.483622Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maraw/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page1.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page2.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page3.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page4.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page5.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page6.png\n"
     ]
    }
   ],
   "source": [
    "from pytesseract import Output\n",
    "import pytesseract\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# process the images to a unified and for an VLM better suiting format\n",
    "def process_image(image, max_size, output_folder, file_path, i):\n",
    "    width, height = image.size\n",
    "    resized_image = resize_image(image, max_size)\n",
    "    rotate_image = correct_text_orientation(resized_image, output_folder, file_path, i)\n",
    "    jpeg_image = convert_to_jpeg(rotate_image)\n",
    "    base64_encoded_image = base64.b64encode(jpeg_image).decode(\"utf-8\")\n",
    "    return (\n",
    "        base64_encoded_image,\n",
    "        max(width, height),\n",
    "    )\n",
    "\n",
    "\n",
    "# most VLM models struggle with rotated text therefore, rotated text gets detect and the pages flipped\n",
    "def correct_text_orientation(image, save_directory, file_path, i):\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = pil_to_cv2(image)\n",
    "\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pytesseract.image_to_osd(rgb, output_type=Output.DICT)\n",
    "\n",
    "    rotated = imutils.rotate_bound(image, angle=results[\"rotate\"])\n",
    "\n",
    "    base_filename = os.path.basename(file_path)\n",
    "    name_without_ext, _ = os.path.splitext(base_filename)\n",
    "    new_filename = os.path.join(\n",
    "        save_directory, f\"corrected_{name_without_ext}_page{i+1}.png\"\n",
    "    )\n",
    "\n",
    "    cv2.imwrite(new_filename, rotated)\n",
    "    print(f\"[INFO] {file_path} - corrected image saved as {new_filename}\")\n",
    "    return rotated\n",
    "\n",
    "\n",
    "# the images get converted into jpeg format\n",
    "def convert_to_jpeg(cv2_image):\n",
    "    retval, buffer = cv2.imencode(\".jpg\", cv2_image)\n",
    "    if retval:\n",
    "        return buffer\n",
    "\n",
    "\n",
    "# conversion of the images from a python-image-library object to an OpenCV object\n",
    "def pil_to_cv2(image):\n",
    "    np_image = np.array(image)\n",
    "    cv2_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR)\n",
    "    return cv2_image\n",
    "\n",
    "\n",
    "# the images get resized to a unified size with a maximum dimensions\n",
    "def resize_image(image, max_dimension):\n",
    "    width, height = image.size\n",
    "\n",
    "    # Check if the image has a palette and convert it to true color mode\n",
    "    if image.mode == \"P\":\n",
    "        if \"transparency\" in image.info:\n",
    "            image = image.convert(\"RGBA\")\n",
    "        else:\n",
    "            image = image.convert(\"RGB\")\n",
    "    # convert to black and white\n",
    "    image = image.convert(\"L\")\n",
    "\n",
    "    if width > max_dimension or height > max_dimension:\n",
    "        if width > height:\n",
    "            new_width = max_dimension\n",
    "            new_height = int(height * (max_dimension / width))\n",
    "        else:\n",
    "            new_height = max_dimension\n",
    "            new_width = int(width * (max_dimension / height))\n",
    "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "output_folder_images = \"./images\"\n",
    "\n",
    "# all images get preprocessed\n",
    "images_base64 = [\n",
    "    process_image(image, 2048, output_folder_images, file_path, j)[0]\n",
    "    for j, image in enumerate(pdf_images)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd238c06473113b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As a next step one could call the OpenAI API. Therefore, one needs an API-key to pay for the calls. Moreover, one needs to create the prompt including the images and the text prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b4943b2bf26a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```{admonition} Prompt\n",
    ":class: tip\n",
    "\n",
    "This is a very simple example prompt. One should optimize and engineer the prompt before usage. For that one could use a tool like [DSPy](https://github.com/stanfordnlp/dspy).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95a25d2099f080d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T10:42:19.111092Z",
     "start_time": "2024-06-03T10:42:19.104520Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the text prompt text for the model call gets defined\n",
    "prompt_text = \"Extract all the relevant information about Buchwald-Hartwig reactions included in these images.\"\n",
    "\n",
    "\n",
    "# the composite prompt is put together\n",
    "def get_prompt_vision_model(images_base64, prompt_text):\n",
    "    content = []\n",
    "    # the images get added in base64 format and in the end the text prompt will be added\n",
    "    for data in images_base64:\n",
    "        content.append(create_image_content(data))\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": prompt_text})\n",
    "    return content\n",
    "\n",
    "\n",
    "# the images get converted into base64 format\n",
    "def create_image_content(image, detail=\"high\"):\n",
    "    return {\n",
    "        \"type\": \"image_url\",\n",
    "        # the level of detail is set to 'high' since mostly text on the images is small\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\", \"detail\": detail},\n",
    "    }\n",
    "\n",
    "\n",
    "# the composite prompt for the model call gets defined\n",
    "prompt = get_prompt_vision_model(images_base64, prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21daa806c07c25b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To call the actual model one could use [LiteLLM](https://github.com/BerriAI/litellm) instead of directly using an API like the OpenAI-API. So one could easily switch between different models for different publishers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11896b70b6aa6a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```{margin}\n",
    "One has to provide his or her own API-key in the `.env` file. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d37e685fef815ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T10:56:02.715950Z",
     "start_time": "2024-06-03T10:55:46.810446Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Here is the extracted information about Buchwald-Hartwig reactions from the provided images:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Buchwald-Hartwig Reactions\": {\n",
      "    \"Key Step\": \"Cross-coupling reaction of an α-amino-BODIPY and the respective halide.\",\n",
      "    \"Conditions\": [\n",
      "      {\n",
      "        \"Reagents\": [\n",
      "          \"Pd(OAc)2\",\n",
      "          \"(±)-BINAP\",\n",
      "          \"Cs2CO3\",\n",
      "          \"PhMe\"\n",
      "        ],\n",
      "        \"Temperature\": \"80 °C\",\n",
      "        \"Time\": \"1-5 h\",\n",
      "        \"Yield\": \"up to 68%\"\n",
      "      }\n",
      "    ],\n",
      "    \"Monomers\": [\n",
      "      {\n",
      "        \"Type\": \"α-chlorinated or α-amino-BODIPYs\",\n",
      "        \"Reagents\": [\n",
      "          \"Pd(OAc)2\",\n",
      "          \"(±)-BINAP\",\n",
      "          \"Cs2CO3\",\n",
      "          \"PhMe\"\n",
      "        ],\n",
      "        \"Temperature\": \"80 °C\",\n",
      "        \"Time\": \"1-5 h\",\n",
      "        \"Yield\": \"up to 68%\"\n",
      "      }\n",
      "    ],\n",
      "    \"Functionalized Monomers\": [\n",
      "      {\n",
      "        \"Type\": \"Br-Ar-mono-Br or Br-Ar-di\",\n",
      "        \"Reagents\": [\n",
      "          \"Pd(OAc)2\",\n",
      "          \"(±)-BINAP\",\n",
      "          \"Cs2CO3\",\n",
      "          \"PhMe\"\n",
      "        ],\n",
      "        \"Temperature\": \"80 °C\",\n",
      "        \"Time\": \"1-5 h\",\n",
      "        \"Yield\": \"44% for Br-Ar-mono-Br, 45% of starting material recovered\"\n",
      "      }\n",
      "    ],\n",
      "    \"Procedure\": \"Stirring slow addition of Br-Ar-mono-NH2 to a heated solution of the remaining reagents.\",\n",
      "    \"Selectivity\": \"Maintained excess of Br-Ar-mono-Br to avoid further oligomerization.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "Input tokens used: 6704 Output tokens used: 387\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "\n",
    "# Define the function to call the LiteLLM API\n",
    "def call_litellm(prompt, model=\"gpt-4o\", temperature: float = 0.0, **kwargs):\n",
    "    \"\"\"Call LiteLLM model\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Prompt to send to model\n",
    "        model (str, optional): Name of the API. Defaults to \"gpt-4o\".\n",
    "        temperature (float, optional): Inference temperature. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: New data\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a scientific assistant, extracting important information about reaction conditions \"\n",
    "                \"out of PDFs in valid JSON format. Extract just data which you are 100% confident about the \"\n",
    "                \"accuracy. Keep the entries short without details. Be careful with numbers.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # Extract and return the message content and token usage\n",
    "    message_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    input_tokens = response[\"usage\"][\"prompt_tokens\"]\n",
    "    output_tokens = response[\"usage\"][\"completion_tokens\"]\n",
    "    return message_content, input_tokens, output_tokens\n",
    "\n",
    "\n",
    "# Load the OpenAI API key from environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the API key for LiteLLM\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# Call the LiteLLM API and print the output and token usage\n",
    "output, input_tokens, output_tokens = call_litellm(prompt=prompt)\n",
    "print(\"Output: \", output)\n",
    "print(\"Input tokens used:\", input_tokens, \"Output tokens used:\", output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca2e31fcdbed75",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```{tip}\n",
    "To get only the json part of the output, one could use Regex to extract this content. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3e84b10a7ecba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since in the article there is no experimental section provided the model just extracted general information about the reactions. It failed with extraction the data provided in the reaction schemes. To extract this information, one should use tools presented in the [agentic section](link to agentic section).\n",
    "\n",
    "Now one could use this structured output to build up a database of Buchwald-Hartwig-Coupling reactions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
