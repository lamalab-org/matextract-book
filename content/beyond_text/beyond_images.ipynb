{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da701e21fecca2a0",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#  Beyond text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5e72798406fa1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP-LLMs tend to have problems with analyzing and understanding complex structures such as tables, plots and images included in scientific articles. Since especially in chemistry and material science information about chemical components is included in these, one should think about different approaches for these structures. Therefore, vision language models (VLMs) since they can analyse images alongside text. There are several open and closed-source VLMs available, e.g., [Vision models from OpenAI](https://platform.openai.com/docs/guides/vision), [Claude models](https://docs.anthropic.com/en/docs/vision) and [DeepSeek-VL](https://github.com/deepseek-ai/DeepSeek-VL). As an example, we show the extraction of images with [GPT4-o](https://platform.openai.com/docs/models/gpt-4o).\n",
    "\n",
    "First one has to convert the file into images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d590539057c1dc3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```{note}\n",
    "\n",
    "The used PDF file was obtained in the [mining data notebook](../obtaining_data/data_mining.ipynb).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b886d18ec7e86797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T10:42:11.479846Z",
     "start_time": "2024-06-03T10:42:10.275394Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matextract  # noqa: F401\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "file_path = \"../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf\"\n",
    "\n",
    "# converting the PDF files to images\n",
    "pdf_images = convert_from_path(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d2d2c714687fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, one should process the obtained images, for instance, rotate pages with vertical text since many models have problems with this. \n",
    "\n",
    "```{admonition} Correcting text orientation \n",
    ":class: dropdown note \n",
    "\n",
    "The algorithm we use here, applies the following steps: \n",
    "\n",
    "1. Convert the image to grayscale.\n",
    "2. Detect edges using the [Canny edge detection algorithm](https://en.wikipedia.org/wiki/Canny_edge_detector).\n",
    "3. Use [Hough Line Transform](https://en.wikipedia.org/wiki/Hough_transform) to detect lines in the image.\n",
    "4. Calculate the angles of these lines.\n",
    "5. Find the dominant angle by taking the median of all angles.\n",
    "6. Based on the dominant angle, determine if the image needs to be rotated 90, 180, or 270 degrees.\n",
    "7. Rotate the image accordingly.\n",
    "```\n",
    "\n",
    "```{admonition} Alternative implementation \n",
    ":class: dropdown note \n",
    "\n",
    "You could implement the preprocessing for text-orientation also with the popular [`tesseract` package](https://github.com/tesseract-ocr/tesseract). \n",
    "`tesseract`'s `image_to_osd` (Orientation and Script Detection) function is specifically designed to detect text orientation, including cases where text might be rotated 90, 180, or 270 degrees. It can identify the script (e.g., Latin, Cyrillic, Arabic) used in the document, which can be useful for multi-language documents and can often handle documents with mixed orientations or complex layouts better than simpler edge-detection methods.\n",
    "\n",
    "    ```{python}\n",
    "    def correct_text_orientation(image, save_directory, file_path, i):\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = pil_to_cv2(image)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pytesseract.image_to_osd(rgb, output_type=Output.DICT)\n",
    "        rotated = imutils.rotate_bound(image, angle=results[\"rotate\"])\n",
    "        base_filename = os.path.basename(file_path)\n",
    "        name_without_ext, * = os.path.splitext(base*filename)\n",
    "        new_filename = os.path.join(\n",
    "            savedirectory, f\"corrected{name_without_ext}_page{i+1}.png\"\n",
    "        )\n",
    "        cv2.imwrite(new_filename, rotated)\n",
    "        print(f\"[INFO] {file_path} - corrected image saved as {new_filename}\")\n",
    "        return rotated\n",
    "    ```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b39a0896010c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T10:42:19.110545Z",
     "start_time": "2024-06-03T10:42:11.483622Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# most VLM models struggle with rotated text therefore, rotated text gets detect and the pages flipped\n",
    "def correct_text_orientation(image, save_directory, file_path, i):\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = pil_to_cv2(image)\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # This applies Canny edge detection to our grayscale image, with lower threshold 50, upper threshold 150, and an aperture size of 3.\n",
    "    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "\n",
    "    # find lines in the edge-detected image, which often correspond to text lines or document edges\n",
    "    lines = cv2.HoughLinesP(\n",
    "        edges, 1, np.pi / 180, 100, minLineLength=100, maxLineGap=10\n",
    "    )\n",
    "\n",
    "    angles = []\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        # The slope of a line is essentially rise over run, or (y2-y1)/(x2-x1). This ratio is exactly what arctan converts into an angle\n",
    "        angle = np.arctan2(y2 - y1, x2 - x1) * 180.0 / np.pi\n",
    "        angles.append(angle)\n",
    "\n",
    "    # Find the dominant angle\n",
    "    dominant_angle = np.median(angles)\n",
    "\n",
    "    # Determine if the image needs to be rotated 90, 180, or 270 degrees\n",
    "    if abs(dominant_angle) < 45:\n",
    "        rotation_angle = 0\n",
    "    elif 45 <= dominant_angle < 135:\n",
    "        rotation_angle = 90\n",
    "    elif -135 <= dominant_angle < -45:\n",
    "        rotation_angle = -90\n",
    "    else:\n",
    "        rotation_angle = 180\n",
    "\n",
    "    rotated = imutils.rotate_bound(image, angle=rotation_angle)\n",
    "\n",
    "    base_filename = os.path.basename(file_path)\n",
    "    name_without_ext, _ = os.path.splitext(base_filename)\n",
    "    new_filename = os.path.join(\n",
    "        save_directory, f\"corrected_{name_without_ext}_page{i+1}.png\"\n",
    "    )\n",
    "    cv2.imwrite(new_filename, rotated)\n",
    "    print(f\"[INFO] {file_path} - corrected image saved as {new_filename}\")\n",
    "    return rotated\n",
    "\n",
    "\n",
    "# the images get converted into jpeg format\n",
    "def convert_to_jpeg(cv2_image):\n",
    "    retval, buffer = cv2.imencode(\".jpg\", cv2_image)\n",
    "    if retval:\n",
    "        return buffer\n",
    "\n",
    "\n",
    "# conversion of the images from a python-image-library object to an OpenCV object\n",
    "def pil_to_cv2(image):\n",
    "    np_image = np.array(image)\n",
    "    cv2_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR)\n",
    "    return cv2_image\n",
    "\n",
    "\n",
    "# the images get resized to a unified size with a maximum dimensions\n",
    "def resize_image(image, max_dimension):\n",
    "    width, height = image.size\n",
    "\n",
    "    # Check if the image has a palette and convert it to true color mode\n",
    "    if image.mode == \"P\":\n",
    "        if \"transparency\" in image.info:\n",
    "            image = image.convert(\"RGBA\")\n",
    "        else:\n",
    "            image = image.convert(\"RGB\")\n",
    "    # convert to black and white\n",
    "    image = image.convert(\"L\")\n",
    "\n",
    "    if width > max_dimension or height > max_dimension:\n",
    "        if width > height:\n",
    "            new_width = max_dimension\n",
    "            new_height = int(height * (max_dimension / width))\n",
    "        else:\n",
    "            new_height = max_dimension\n",
    "            new_width = int(width * (max_dimension / height))\n",
    "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d3ea8",
   "metadata": {},
   "source": [
    "Next, one has to convert the pictures into machine-readable Base-64 format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb3233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page1.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page2.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page3.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page4.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page5.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page6.png\n"
     ]
    }
   ],
   "source": [
    "# process the images to a unified and for an VLM better suiting format\n",
    "def process_image(image, max_size, output_folder, file_path, i):\n",
    "    width, height = image.size\n",
    "    resized_image = resize_image(image, max_size)\n",
    "    rotate_image = correct_text_orientation(resized_image, output_folder, file_path, i)\n",
    "    jpeg_image = convert_to_jpeg(rotate_image)\n",
    "    base64_encoded_image = base64.b64encode(jpeg_image).decode(\"utf-8\")\n",
    "    return (\n",
    "        base64_encoded_image,\n",
    "        max(width, height),\n",
    "    )\n",
    "\n",
    "\n",
    "output_folder_images = \"./images\"\n",
    "\n",
    "# all images get preprocessed\n",
    "images_base64 = [\n",
    "    process_image(image, 2048, output_folder_images, file_path, j)[0]\n",
    "    for j, image in enumerate(pdf_images)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd238c06473113b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As a next step, one could call the OpenAI API. Therefore, one needs an API-key to pay for the calls. Moreover, one needs to create the prompt including the images and the text prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b4943b2bf26a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```{admonition} Prompt\n",
    ":class: tip\n",
    "\n",
    "This is a very simple example prompt. One should optimize and engineer the prompt before usage. For that one could use a tool like [DSPy](https://github.com/stanfordnlp/dspy).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95a25d2099f080d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T10:42:19.111092Z",
     "start_time": "2024-06-03T10:42:19.104520Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the text prompt text for the model call gets defined\n",
    "prompt_text = \"Extract all the relevant information about Buchwald-Hartwig reactions included in these images.\"\n",
    "\n",
    "\n",
    "# the composite prompt is put together\n",
    "def get_prompt_vision_model(images_base64, prompt_text):\n",
    "    content = []\n",
    "    # the images get added in base64 format and in the end the text prompt will be added\n",
    "    for data in images_base64:\n",
    "        content.append(create_image_content(data))\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": prompt_text})\n",
    "    return content\n",
    "\n",
    "\n",
    "# the images get converted into base64 format\n",
    "def create_image_content(image, detail=\"high\"):\n",
    "    return {\n",
    "        \"type\": \"image_url\",\n",
    "        # the level of detail is set to 'high' since mostly text on the images is small\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\", \"detail\": detail},\n",
    "    }\n",
    "\n",
    "\n",
    "# the composite prompt for the model call gets defined\n",
    "prompt = get_prompt_vision_model(images_base64, prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21daa806c07c25b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To call the actual model one could use [LiteLLM](https://github.com/BerriAI/litellm) instead of directly using an API like the OpenAI-API. So one could easily switch between different models for different providers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11896b70b6aa6a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```{margin}\n",
    "One has to provide his or her own API-key in the `.env` file. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d37e685fef815ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T10:56:02.715950Z",
     "start_time": "2024-06-03T10:55:46.810446Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Here is the extracted information about Buchwald-Hartwig reactions from the provided images:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Buchwald-Hartwig Reactions\": {\n",
      "    \"Key Step\": \"Cross-coupling reaction of an α-amino-BODIPY and the respective halide.\",\n",
      "    \"Conditions\": [\n",
      "      {\n",
      "        \"Reagents\": [\n",
      "          \"Pd(OAc)2\",\n",
      "          \"(±)-BINAP\",\n",
      "          \"Cs2CO3\",\n",
      "          \"PhMe\"\n",
      "        ],\n",
      "        \"Temperature\": \"80 °C\",\n",
      "        \"Time\": \"1.5 h\"\n",
      "      },\n",
      "      {\n",
      "        \"Reagents\": [\n",
      "          \"Pd(OAc)2\",\n",
      "          \"(±)-BINAP\",\n",
      "          \"Cs2CO3\",\n",
      "          \"PhMe\"\n",
      "        ],\n",
      "        \"Temperature\": \"80 °C\",\n",
      "        \"Time\": \"6-322 h\"\n",
      "      }\n",
      "    ],\n",
      "    \"Yields\": [\n",
      "      {\n",
      "        \"Compound\": \"EDM-Ar-mono-NH2\",\n",
      "        \"Yield\": \"47%\"\n",
      "      },\n",
      "      {\n",
      "        \"Compound\": \"DM-Ar-mono-NH2\",\n",
      "        \"Yield\": \"58%\"\n",
      "      },\n",
      "      {\n",
      "        \"Compound\": \"Br-Ar-mono-NH2\",\n",
      "        \"Yield\": \"56%\"\n",
      "      },\n",
      "      {\n",
      "        \"Compound\": \"EDM-Ar-di\",\n",
      "        \"Yield\": \"20%\"\n",
      "      },\n",
      "      {\n",
      "        \"Compound\": \"DM-Ar-di\",\n",
      "        \"Yield\": \"30%\"\n",
      "      },\n",
      "      {\n",
      "        \"Compound\": \"Br-Ar-di\",\n",
      "        \"Yield\": \"44%\"\n",
      "      },\n",
      "      {\n",
      "        \"Compound\": \"EDM-Ar-di\",\n",
      "        \"Yield\": \"66%\"\n",
      "      },\n",
      "      {\n",
      "        \"Compound\": \"DM-Ar-di\",\n",
      "        \"Yield\": \"62%\"\n",
      "      },\n",
      "      {\n",
      "        \"Compound\": \"Br-Ar-di\",\n",
      "        \"Yield\": \"45%\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n",
      "Input tokens used: 6704 Output tokens used: 389\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "\n",
    "# Define the function to call the LiteLLM API\n",
    "def call_litellm(prompt, model=\"gpt-4o\", temperature: float = 0.0, **kwargs):\n",
    "    \"\"\"Call LiteLLM model\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Prompt to send to model\n",
    "        model (str, optional): Name of the API. Defaults to \"gpt-4o\".\n",
    "        temperature (float, optional): Inference temperature. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: New data\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a scientific assistant, extracting important information about reaction conditions \"\n",
    "                \"out of PDFs in valid JSON format. Extract just data which you are 100% confident about the \"\n",
    "                \"accuracy. Keep the entries short without details. Be careful with numbers.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # Extract and return the message content and token usage\n",
    "    message_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    input_tokens = response[\"usage\"][\"prompt_tokens\"]\n",
    "    output_tokens = response[\"usage\"][\"completion_tokens\"]\n",
    "    return message_content, input_tokens, output_tokens\n",
    "\n",
    "\n",
    "# Call the LiteLLM API and print the output and token usage\n",
    "output, input_tokens, output_tokens = call_litellm(prompt=prompt)\n",
    "print(\"Output: \", output)\n",
    "print(\"Input tokens used:\", input_tokens, \"Output tokens used:\", output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca2e31fcdbed75",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```{tip}\n",
    "To get only the JSON part of the output, one could use Regex to extract this content. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3e84b10a7ecba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since in the article there is no experimental section provided, the model just extracted general information about the reactions. It failed with extraction the data provided in the reaction schemes. To extract this information, one should use tools presented in the [agentic section](../agents/agent.ipynb).\n",
    "\n",
    "Now one could use this structured output to build up a database of Buchwald-Hartwig-Coupling reactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed144b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
