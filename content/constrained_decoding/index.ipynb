{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained generation to guarantee syntactic correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If we want to generate output that is structured in a specific way, we can use various techniques to \n",
    "\n",
    "- make the extraction more efficient (but automatically adding the \"obvious\" tokens)\n",
    "- make the generation guaranteed to be syntactically correct\n",
    "- make the generation sometimes more semantically correct, too \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable constrained decoding, we will use one of the most popular packages for this task [`instructor`](https://jxnl.github.io/instructor/).\n",
    "It is built on [`pydantic`]() and can leverage function calling and JSON-mode of the OpenAI API as well as other constrained sampling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import instructor\n",
    "from litellm import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Importance of constrained decoding \n",
    ":class: important\n",
    "\n",
    "Constrained decoding techniques are important because they allow us to _guarantee_ that the generated data follows a certain structure. \n",
    "When we prompt models to produce data in a certain schema they often do so but sometimes they will not. \n",
    "This behavior makes it difficult to write code that uses the output as it would now need to be able to handle cases in which the model output is not in the expected schema. \n",
    "\n",
    "In addition, it partially also reduces the space for hallucination as we can constrain the pool of possible things the model can return. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a data schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most constrained generation tasks, we need to define a data schema in a programmatic way.\n",
    "The most common way to do so is to use `pydantic` data classes.\n",
    "Here is an example of a simple data schema for a recipe:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    title: str\n",
    "    ingredients: List[str]\n",
    "    instructions: List[str]\n",
    "```\n",
    "\n",
    "This schema can also be extended to include descriptions of different fields or to only allow certain values for specific fields. For example, we could add a field for the number of servings and only allow positive integers.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    title: str\n",
    "    ingredients: List[str]\n",
    "    instructions: List[str]\n",
    "    servings: int = Field(..., gt=0, description=\"The number of servings for this recipe\")\n",
    "    rating: Literal[\"easy\", \"medium\", \"hard\"] = Field(\"easy\", description=\"The difficulty level of this recipe\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to extract reactions a data schema could look like the following.\n",
    "\n",
    "We can now use `instructor` to \"patch\" the OpenAI API client to ensure that our output fulfills the schema ([this works by providing the right response model](https://python.useinstructor.com/hub/anyscale/?h=patch), i.e., using what OpenAI calls [\"function calling\"](https://python.useinstructor.com/hub/anyscale/?h=patch))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n",
    "\n",
    "\n",
    "class Amount(BaseModel):\n",
    "    value: float\n",
    "    units: str\n",
    "\n",
    "\n",
    "class Component(BaseModel):\n",
    "    name: str\n",
    "    amount: Optional[Amount]\n",
    "    reaction_role: str\n",
    "\n",
    "\n",
    "class Inputs(BaseModel):\n",
    "    components: List[Component]\n",
    "\n",
    "\n",
    "class Temperature(BaseModel):\n",
    "    control_type: str\n",
    "    value: float\n",
    "    units: str\n",
    "\n",
    "\n",
    "class Conditions(BaseModel):\n",
    "    temperature: Temperature\n",
    "    conditions_are_dynamic: bool\n",
    "\n",
    "\n",
    "class Workup(BaseModel):\n",
    "    type: str\n",
    "    details: str\n",
    "    duration: Optional[Amount] = None\n",
    "\n",
    "\n",
    "class Measurement(BaseModel):\n",
    "    type: str\n",
    "    details: str\n",
    "    amount: Amount\n",
    "\n",
    "\n",
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    measurements: List[Measurement]\n",
    "    reaction_role: str\n",
    "\n",
    "\n",
    "class Outcome(BaseModel):\n",
    "    products: List[Product]\n",
    "\n",
    "\n",
    "class Reaction(BaseModel):\n",
    "    inputs: Inputs\n",
    "    conditions: Conditions\n",
    "    workups: List[Workup]\n",
    "    outcomes: List[Outcome]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Visualizing the schema\n",
    ":class: tip, dropdown\n",
    "\n",
    "To visualize the schema, we can use the following code:\n",
    "\n",
    "    import erdantic as erd\n",
    "    from IPython.display import SVG\n",
    "    \n",
    "    diagram = erd.create(Reaction)\n",
    "    diagram.draw(\"diagram.svg\")\n",
    "    SVG(\"diagram.svg\")\n",
    "\n",
    "You should then see a diagram like \n",
    "\n",
    "![](diagram.svg)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will use a document we already looked at the document cleaning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"../document_parsing_and_cleaning/markdown_files/10.26434_chemrxiv-2024-1l0sn.mmd\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this text, we can now use the model to extract reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    response_model=List[Reaction],\n",
    "    max_retries=2,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a scientific assistant, extracting accurate information about organic reactions from scientific papers.\n",
    "Do not use data that was reproduced from other sources.\n",
    "NEVER combine data from different reactions, otherwise you will be penalized.\n",
    "If you are unsure, return no data. Quality is more important than quantity.\n",
    "\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Extract the data from the paper into the provided data schema. We want an iterable of reaction objects and each reaction will be its own object.\n",
    "Never return data that you are not absolutely sure about! You will be penalized for incorrect data.\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": data},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Reaction(inputs=Inputs(components=[Component(name='alpha-chlorinated 2-acyl/pyrroles', amount=None, reaction_role='reactant'), Component(name='alkyl/pyrroles', amount=None, reaction_role='reactant'), Component(name='POCl3', amount=None, reaction_role='reagent'), Component(name='CH4Cl/n-hexane (2:1)', amount=None, reaction_role='solvent'), Component(name='triethylamine', amount=None, reaction_role='reagent'), Component(name='BFx-OELx', amount=None, reaction_role='reagent')]), conditions=Conditions(temperature=Temperature(control_type='unspecified', value=0.0, units='degree Celsius'), conditions_are_dynamic=False), workups=[], outcomes=[Outcome(products=[Product(name='N-bridged BODIPY dimers', measurements=[Measurement(type='yield', details='over 2 steps', amount=Amount(value=91.0, units='%'))], reaction_role='product')])]),\n",
       " Reaction(inputs=Inputs(components=[Component(name='Br-Ar-mono-Br', amount=None, reaction_role='reactant'), Component(name='pyrrole', amount=None, reaction_role='reactant'), Component(name='4-iso-butylbenzaldehyde', amount=None, reaction_role='reactant'), Component(name='TFA', amount=None, reaction_role='catalyst'), Component(name='CH4Cl2', amount=None, reaction_role='solvent'), Component(name='NBS', amount=None, reaction_role='reagent'), Component(name='THF', amount=None, reaction_role='solvent'), Component(name='DDQ', amount=None, reaction_role='oxidant')]), conditions=Conditions(temperature=Temperature(control_type='specified', value=-78.0, units='degree Celsius'), conditions_are_dynamic=False), workups=[], outcomes=[Outcome(products=[Product(name='Br-Ar-mono-Br', measurements=[Measurement(type='yield', details='over three steps', amount=Amount(value=38.0, units='%'))], reaction_role='product')])])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output are Python `pydantic` objects that we can easily reuse. \n",
    "The schema can be made more complex, e.g., using `Literal` to only allow certain reaction roles or measurement types.\n",
    "\n",
    "Thus, we recommend the use of constrained decoding techniques for almost all applications since a datamodel must always be defined and the use of this data model in a constrained decoding setup only presents very little overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Further reading \n",
    ":class: tip \n",
    "\n",
    "You can find more tools that help to constrain the LLM output on the [Awesome LLM JSON List](https://github.com/imaurer/awesome-llm-json).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
