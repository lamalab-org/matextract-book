{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Evaluations\"\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every extraction task needs a good way to evaluate whether the extracted data is correct and give it a score of how correct it is. The goal is to quantify the extraction pipeline's (model's) performance. With partial scores giving insight on how correct a data point is, usually between 0-1, the pipeline can be improved by fixing any edge cases or errors found by comparing lower scored data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We import all required libraries at the start here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "from pint import UnitRegistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = {\n",
    "    \"text\": \"result\",\n",
    "    \"correct\": \"correct\",\n",
    "    \"number\": 0.45,\n",
    "    \"wrong\": 1.023,\n",
    "    \"bool\": False,\n",
    "    \"missing\": 0,\n",
    "}\n",
    "prediction = {\n",
    "    \"text\": \"result\",\n",
    "    \"correct\": \"incorrect\",\n",
    "    \"number\": 0.45,\n",
    "    \"wrong\": 1.025,\n",
    "    \"bull\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring a Key-Value pair\n",
    "\n",
    "To properly evaluate a structured output from a model, we have to walk through the structure of the ground truth and the output simultaneously. We have to check every volue we were expecting the model to return and count how many of these are correct.\n",
    "With the count of the correct keys, we can calculate Precision and Recall:\n",
    "\n",
    "precision = (|{relevant entries} ∩ {retrieved entries}|) / (|{retrieved entries}|)\n",
    "\n",
    "\n",
    "recall = (|{relevant entries} ∩ {retrieved entries}|) / (|{relevant entries}|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.0 Precision:  0.0\n"
     ]
    }
   ],
   "source": [
    "relevant_entries_intersection_retrieved_entries = []\n",
    "for t_key in truth:\n",
    "    if t_key in prediction:\n",
    "        if truth[t_key] == prediction[t_key]:\n",
    "            relevant_entries_intersection_retrieved_entries.append(t_key)\n",
    "recall = len(relevant_entries_intersection_retrieved_entries) / len(truth.keys())\n",
    "precision = len(relevant_entries_intersection_retrieved_entries) / len(\n",
    "    prediction.keys()\n",
    ")\n",
    "print(\"Recall: \", recall, \"Precision: \", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of results from the Choosing a learning paradigm chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the results\n",
    "We will use the responses from GPT-4o. We will read the JSON file that has both the predicted structured output and the expected reference ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../finetune/OpenAI_results.json\", \"r\") as f:\n",
    "    open_ai_results = json.load(f)\n",
    "    truths = open_ai_results[\"1-shot\"][\"references\"]\n",
    "    predictions = open_ai_results[\"1-shot\"][\"predictions\"]\n",
    "    # We convert the output from a JSON str to a dict object\n",
    "    truths = [json.loads(truth) for truth in truths]\n",
    "    predictions = [\n",
    "        json.loads(prediction[prediction.index(\"{\") : prediction.rindex(\"}\") + 1])\n",
    "        for prediction in predictions\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining common functions for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_items(dict1, dict2):\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            if isinstance(dict1[key], dict) and isinstance(dict2[key], dict):\n",
    "                for subkey, subvalue in common_items(dict1[key], dict2[key]):\n",
    "                    yield f\"{key}.{subkey}\", subvalue\n",
    "            elif isinstance(dict1[key], list) and isinstance(dict2[key], list):\n",
    "                for item1 in dict1[key]:\n",
    "                    for item2 in dict2[key]:\n",
    "                        if isinstance(item1, dict) and isinstance(item2, dict):\n",
    "                            for subkey, subvalue in common_items(item1, item2):\n",
    "                                yield f\"{key}.[].{subkey}\", subvalue\n",
    "            elif dict1[key] == dict2[key]:\n",
    "                yield key, dict1[key]\n",
    "\n",
    "\n",
    "def count_correct_items(dict1, dict2):\n",
    "    return len([correct for correct in common_items(dict1, dict2)])\n",
    "\n",
    "\n",
    "def count_leaf_keys(d):\n",
    "    if isinstance(d, dict):\n",
    "        count = 0\n",
    "        for value in d.values():\n",
    "            count += count_leaf_keys(value)\n",
    "        return count\n",
    "    elif isinstance(d, list):\n",
    "        count = 0\n",
    "        for item in d:\n",
    "            count += count_leaf_keys(item)\n",
    "        return count\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29688510133403073 0.2954236615993505\n"
     ]
    }
   ],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "for i, truth in enumerate(truths):\n",
    "    prediction = predictions[i]\n",
    "    num_correct_items = count_correct_items(truth, prediction)\n",
    "    recalls.append(num_correct_items / count_leaf_keys(truth))\n",
    "    precisions.append(num_correct_items / count_leaf_keys(prediction))\n",
    "\n",
    "print(mean(recalls), mean(precisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "::: {.column-margin}\n",
    "The variable `truth` is the manually curated reference extraction and `prediction` is what the llm returns through the pipeline.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.36363636363636365\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2 / ((1 / recall) + (1 / precision))\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching to ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets re-run the evaluation on just the **inputs** section of our schema. We will use this as an example to show how using a fuzzy matching can improve our scores. Sometimes, the model misinterprets part of the structure but still understands values deeply nested in this structure. Matching allows us to score the misinterpreted section with potentially correct subsections.\n",
    "\n",
    "We run the same evaluation routine but instead of looking for correct keys in the whole section we limit it to the **inputs** section.\n",
    "\n",
    "You can see the improvement in the evaluation scores between the two blocks of code. Matching can give you a better understanding of where and how your model's predictions need improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1282237586783771 0.11026275208112904\n"
     ]
    }
   ],
   "source": [
    "from thefuzz import fuzz\n",
    "from munkres import Munkres\n",
    "\n",
    "m = Munkres()\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "for i, truth in enumerate(truths):\n",
    "    prediction = predictions[i]\n",
    "    num_correct_items = count_correct_items(\n",
    "        truth[\"inputs\"], prediction[\"inputs\"]\n",
    "    )  # Here we select just the \"inputs\"\n",
    "    recalls.append(num_correct_items / count_leaf_keys(truth[\"inputs\"]))\n",
    "    precisions.append(num_correct_items / count_leaf_keys(prediction[\"inputs\"]))\n",
    "\n",
    "print(mean(recalls), mean(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6935011404798668 0.6244815185295368\n"
     ]
    }
   ],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "m = Munkres()\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "for i, truth in enumerate(truths):\n",
    "    prediction = predictions[i]\n",
    "    scores = [\n",
    "        [fuzz.token_sort_ratio(t, p) for t in truth[\"inputs\"]]\n",
    "        for p in prediction[\"inputs\"]\n",
    "    ]\n",
    "    indexes = m.compute(scores)\n",
    "    matches = [\n",
    "        (list(truth[\"inputs\"].values())[t], list(prediction[\"inputs\"].values())[p])\n",
    "        for p, t in indexes\n",
    "    ]\n",
    "    num_correct_items = count_correct_items(matches[0][0], matches[0][1])\n",
    "    recalls.append(num_correct_items / count_leaf_keys(truth[\"inputs\"]))\n",
    "    precisions.append(num_correct_items / count_leaf_keys(prediction[\"inputs\"]))\n",
    "\n",
    "print(mean(recalls), mean(precisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "Sometimes the data you have in the ground truth could be normalized to a certain format. For example, all units could be converted to SI units. The original text might have them in different units. In such a situation, it is better to convert the prediction from the model to our required units before checking those keys.\n",
    "\n",
    "Let's take a **truth** and **prediction** consisting of values and units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = {\"voltage\": {\"value\": 22.0, \"unit\": \"V\"}}\n",
    "\n",
    "prediction = {\"voltage\": {\"value\": 22000.0, \"unit\": \"mV\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can parse the prediction with **pint** and normalize the values to SI units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 22000.0 mV\n",
      "to 22.0 volt\n"
     ]
    }
   ],
   "source": [
    "ureg = UnitRegistry()\n",
    "text_representation_of_value = (\n",
    "    str(prediction[\"voltage\"][\"value\"]) + \" \" + prediction[\"voltage\"][\"unit\"]\n",
    ")\n",
    "print(\"Converting\", text_representation_of_value)\n",
    "normalized_pint_quantity = ureg(text_representation_of_value).to(\"V\")\n",
    "print(\"to\", normalized_pint_quantity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the magnitudes of our truth value and our normalized predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value is correct.\n"
     ]
    }
   ],
   "source": [
    "if truth[\"voltage\"][\"value\"] == normalized_pint_quantity.magnitude:\n",
    "    print(\"Predicted value is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
