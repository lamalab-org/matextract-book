{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every extraction task needs a good way to evaluate whether the extracted data is correct and give it a score of how correct it is. The goal is to quantify the extraction pipeline's (model's) performance. With partial scores giving insight on how correct a data point is, usually between 0-1, the pipeline can be improved by fixing any edge cases or errors found by comparing lower scored data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We import all required libraries at the start here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "from pint import UnitRegistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example data\n",
    "\n",
    "To start of with a simple example, we define a set of key-value pairs as our ground truth and a dummy output from the model as prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = {\n",
    "    \"text\": \"result\",\n",
    "    \"correct\": \"correct\",\n",
    "    \"number\": 0.45,\n",
    "    \"wrong\": 1.023,\n",
    "    \"bool\": False,\n",
    "    \"missing\": 0,\n",
    "}\n",
    "prediction = {\n",
    "    \"text\": \"result\",\n",
    "    \"correct\": \"incorrect\",\n",
    "    \"number\": 0.45,\n",
    "    \"wrong\": 1.025,\n",
    "    \"bull\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring a Key-Value pair\n",
    "\n",
    "To properly evaluate a structured output from a model, we have to walk through the structure of the ground truth and the output simultaneously. We have to check every value we were expecting the model to return and count how many of these are correct.\n",
    "With the count of the correct keys, we can calculate our metrics:\n",
    "\n",
    "precision = (|{relevant entries} ∩ {retrieved entries}|) / (|{retrieved entries}|)\n",
    "\n",
    "\n",
    "recall = (|{relevant entries} ∩ {retrieved entries}|) / (|{relevant entries}|)\n",
    "\n",
    "\n",
    "F1-Score = 2 / ((1 / recall) + (1 / precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.33\n",
      "Precision: 0.40\n"
     ]
    }
   ],
   "source": [
    "relevant_entries_intersection_retrieved_entries = []\n",
    "for t_key in truth:\n",
    "    if t_key in prediction:\n",
    "        if truth[t_key] == prediction[t_key]:\n",
    "            relevant_entries_intersection_retrieved_entries.append(t_key)\n",
    "recall = len(relevant_entries_intersection_retrieved_entries) / len(truth.keys())\n",
    "precision = len(relevant_entries_intersection_retrieved_entries) / len(\n",
    "    prediction.keys()\n",
    ")\n",
    "print(f\"Recall: {recall:.2f}\\nPrecision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of results from the [Choosing a learning paradigm chapter](choosing-paradigm-openai-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the results\n",
    "We will use the responses from GPT-4o. We will read the JSON file that has both the predicted structured output and the expected reference ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../finetune/OpenAI_results.json\", \"r\") as f:\n",
    "    open_ai_results = json.load(f)\n",
    "    truths = open_ai_results[\"1-shot\"][\"references\"]\n",
    "    predictions = open_ai_results[\"1-shot\"][\"predictions\"]\n",
    "    # We convert the output from a JSON str to a dict object\n",
    "    truths = [json.loads(truth) for truth in truths]\n",
    "    predictions = [\n",
    "        json.loads(prediction[prediction.index(\"{\") : prediction.rindex(\"}\") + 1])\n",
    "        for prediction in predictions\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining common functions for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_items(dict1, dict2):\n",
    "    \"\"\"Finds common keys between two dicts and yields if the values are equal.\"\"\"\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            if isinstance(dict1[key], dict) and isinstance(dict2[key], dict):\n",
    "                for subkey, subvalue in common_items(dict1[key], dict2[key]):\n",
    "                    yield f\"{key}.{subkey}\", subvalue\n",
    "            elif isinstance(dict1[key], list) and isinstance(dict2[key], list):\n",
    "                for item1 in dict1[key]:\n",
    "                    for item2 in dict2[key]:\n",
    "                        if isinstance(item1, dict) and isinstance(item2, dict):\n",
    "                            for subkey, subvalue in common_items(item1, item2):\n",
    "                                yield f\"{key}.[].{subkey}\", subvalue\n",
    "            elif dict1[key] == dict2[key]:\n",
    "                yield key, dict1[key]\n",
    "\n",
    "\n",
    "def count_correct_items(dict1, dict2):\n",
    "    \"\"\"Counts the common keys between two dicts and returns the count.\"\"\"\n",
    "    return len([correct for correct in common_items(dict1, dict2)])\n",
    "\n",
    "\n",
    "def count_leaf_keys(d):\n",
    "    \"\"\"Counts the number of keys at the deepest levels (leaf) of an arbitrarily nested dict\"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        count = 0\n",
    "        for value in d.values():\n",
    "            count += count_leaf_keys(value)\n",
    "        return count\n",
    "    elif isinstance(d, list):\n",
    "        count = 0\n",
    "        for item in d:\n",
    "            count += count_leaf_keys(item)\n",
    "        return count\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.30\n",
      "Precision: 0.30\n"
     ]
    }
   ],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "for i, truth in enumerate(truths):\n",
    "    prediction = predictions[i]\n",
    "    num_correct_items = count_correct_items(truth, prediction)\n",
    "    recalls.append(num_correct_items / count_leaf_keys(truth))\n",
    "    precisions.append(num_correct_items / count_leaf_keys(prediction))\n",
    "\n",
    "recall = mean(recalls)\n",
    "precision = mean(precisions)\n",
    "print(f\"Recall: {recall:.2f}\\nPrecision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```{note}\n",
    "The variable `truth` is the manually curated reference extraction and `prediction` is what the llm returns through the pipeline.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.30\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2 / ((1 / recall) + (1 / precision))\n",
    "print(f\"F1 score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching to ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets re-run the evaluation on just the **inputs** section of our schema. We will use this as an example to show how using a fuzzy matching can improve our scores. Sometimes, the model misinterprets part of the structure but still understands values deeply nested in this structure. Matching allows us to score the misinterpreted section with potentially correct subsections.\n",
    "\n",
    "We run the same evaluation routine but instead of looking for correct keys in the whole section we limit it to the **inputs** section.\n",
    "\n",
    "You can see the improvement in the evaluation scores between the two blocks of code. Matching can give you a better understanding of where and how your model's predictions need improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.13\n",
      "Precision: 0.11\n"
     ]
    }
   ],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "for i, truth in enumerate(truths):\n",
    "    prediction = predictions[i]\n",
    "    num_correct_items = count_correct_items(\n",
    "        truth[\"inputs\"], prediction[\"inputs\"]\n",
    "    )  # Here we select just the \"inputs\"\n",
    "    recalls.append(num_correct_items / count_leaf_keys(truth[\"inputs\"]))\n",
    "    precisions.append(num_correct_items / count_leaf_keys(prediction[\"inputs\"]))\n",
    "\n",
    "recall = mean(recalls)\n",
    "precision = mean(precisions)\n",
    "print(f\"Recall: {recall:.2f}\\nPrecision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.69\n",
      "Precision: 0.62\n"
     ]
    }
   ],
   "source": [
    "from munkres import Munkres\n",
    "from thefuzz import fuzz\n",
    "\n",
    "m = Munkres()  # We will use the  Kuhn-Munkres algorithm for our matching\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "for i, truth in enumerate(truths):\n",
    "    prediction = predictions[i]\n",
    "    # Here we use a 2d matrix to store the string comparison score of every key from truth and predictions\n",
    "    scores = [\n",
    "        [fuzz.token_sort_ratio(t, p) for t in truth[\"inputs\"]]\n",
    "        for p in prediction[\"inputs\"]\n",
    "    ]\n",
    "    indexes = m.compute(scores)  # We find the best matching scores for each key\n",
    "    # Once we have the indexes of the matches, we collect the pairs in one list object\n",
    "    matches = [\n",
    "        (list(truth[\"inputs\"].values())[t], list(prediction[\"inputs\"].values())[p])\n",
    "        for p, t in indexes\n",
    "    ]\n",
    "    # Now we can score according to the matches we found.\n",
    "    num_correct_items = count_correct_items(matches[0][0], matches[0][1])\n",
    "    recalls.append(num_correct_items / count_leaf_keys(truth[\"inputs\"]))\n",
    "    precisions.append(num_correct_items / count_leaf_keys(prediction[\"inputs\"]))\n",
    "\n",
    "\n",
    "recall = mean(recalls)\n",
    "precision = mean(precisions)\n",
    "print(f\"Recall: {recall:.2f}\\nPrecision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "Sometimes the data you have in the ground truth could be normalized to a certain format. For example, all units could be converted to SI units. The original text might have them in different units. In such a situation, it is better to convert the prediction from the model to our required units before checking those keys.\n",
    "\n",
    "Let's take a **truth** and **prediction** consisting of values and units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = {\"mass\": {\"value\": 22.0, \"unit\": \"g\"}}\n",
    "\n",
    "prediction = {\"mass\": {\"value\": 22000.0, \"unit\": \"mg\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can parse the prediction with **pint** and normalize the values to SI units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 22000.0 mg\n",
      "to 22.0 gram\n"
     ]
    }
   ],
   "source": [
    "ureg = UnitRegistry()\n",
    "text_representation_of_value = (\n",
    "    str(prediction[\"mass\"][\"value\"]) + \" \" + prediction[\"mass\"][\"unit\"]\n",
    ")\n",
    "print(\"Converting\", text_representation_of_value)\n",
    "normalized_pint_quantity = ureg(text_representation_of_value).to(\"g\")\n",
    "print(\"to\", normalized_pint_quantity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the magnitudes of our truth value and our normalized predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value is correct.\n"
     ]
    }
   ],
   "source": [
    "if truth[\"mass\"][\"value\"] == normalized_pint_quantity.magnitude:\n",
    "    print(\"Predicted value is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chemically informed normalization\n",
    "When we are trying to extract chemical formulas, they could be reported in various forms. To make sure we got the expected value regardless of what form it's represented in, we can convert them to their SMILES representation. This is important to make sure we have the right information and not score incorrectly.\n",
    "\n",
    "Here, we setup example extractions from a model and show how to get their SMILES before validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.80\n",
      "Precision: 1.00\n"
     ]
    }
   ],
   "source": [
    "from llmstructdata.utils import name_to_smiles\n",
    "\n",
    "truth = {\"solvents\": [\"CCCO\", \"CC(C)O\", \"CC(C)=O\", \"CC(=O)O\", \"C=O\"]}\n",
    "\n",
    "prediction = {\"solvents\": [\"propanol\", \"isopropanol\", \"Propanone\", \"Ethanoic acid\"]}\n",
    "\n",
    "predictions_as_smiles = {\n",
    "    \"solvents\": [name_to_smiles(solvent) for solvent in prediction[\"solvents\"]]\n",
    "}\n",
    "\n",
    "number_of_values_correct = len(\n",
    "    [\n",
    "        solvent\n",
    "        for solvent in truth[\"solvents\"]\n",
    "        if solvent in predictions_as_smiles[\"solvents\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "precision = number_of_values_correct / len(predictions_as_smiles[\"solvents\"])\n",
    "recall = number_of_values_correct / len(truth[\"solvents\"])\n",
    "print(f\"Recall: {recall:.2f}\\nPrecision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inorganics\n",
    "Here is how one could do a similar normalization for inorganic substances and normalize between the Hill system, IUPAC, and just a reduced empirical formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.60\n",
      "Precision: 0.75\n"
     ]
    }
   ],
   "source": [
    "from pymatgen.core import Composition\n",
    "\n",
    "truth = {\"inorganics\": [\"SiC2\", \"CaCO3\", \"NaCN\", \"CO\", \"HCL\"]}\n",
    "\n",
    "prediction = {\"inorganics\": [\"C2 Si\", \"C Ca O3\", \"Na1 C1 N1\", \"C1 O6\"]}\n",
    "\n",
    "predictions_as_reduced_formula = {\n",
    "    \"inorganics\": [\n",
    "        Composition(inorganic).reduced_formula for inorganic in prediction[\"inorganics\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "number_of_values_correct = len(\n",
    "    [\n",
    "        inorganic\n",
    "        for inorganic in truth[\"inorganics\"]\n",
    "        if inorganic in predictions_as_reduced_formula[\"inorganics\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "precision = number_of_values_correct / len(predictions_as_smiles[\"inorganics\"])\n",
    "recall = number_of_values_correct / len(truth[\"inorganics\"])\n",
    "print(f\"Recall: {recall:.2f}\\nPrecision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
