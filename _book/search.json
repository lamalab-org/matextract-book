[
  {
    "objectID": "obtaining_data/obtaining_data.html",
    "href": "obtaining_data/obtaining_data.html",
    "title": "1 | Obtaining data for data extraction",
    "section": "",
    "text": "At the start of each data extraction process one has to first collect a dataset containing all data sources relevant for the extraction topic. Various Python packages are available to make this process easier and more efficient.\n\nCrossref can be used to search for relevant articles and metadata\nData can be mined from various sources such as ChemRxiv",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>1 | Obtaining data for data extraction</span>"
    ]
  },
  {
    "objectID": "obtaining_data/crossref_search.html",
    "href": "obtaining_data/crossref_search.html",
    "title": "1.1 | Obtaining a set of relevant data sources",
    "section": "",
    "text": "At the start of the data extraction process you have to collect a set of potentially relevant data sources. Therefore, you could collect a dataset manually or use a tool to help to automate and speed up this process.\nThe Crossref API is a very useful tool to collect the metadata of relevant articles. Besides the API there are multiple Python libraries available that make access to the API easier. One of these libraries is crossrefapi. As an example, 100 sources including metadata on the topic ‘buchwald-hartwig coupling’ are extracted and saved into a JSON file.\n\nfrom crossref.restful import Works\nimport json\n\nworks = Works(timeout=60)\n\n# Performing the search for sources on the topic of buchwald-hartwig coupling for 10 papers\nquery_result = works.query(bibliographic='buchwald-hartwig coupling').select('DOI', 'title', 'author', 'type', 'publisher', 'issued').sample(10)\n\nresults = [item for item in query_result]\n\n# Save 100 results including their metadata in a json file\nwith open('buchwald-hartwig_coupling_results.json', 'w') as file:\n    json.dump(results, file)\n\nprint(results)\n\nWith the obtained metadata one could afterwards try to filter for relevant or available data sources which could be downloaded through an API provided by the publishers or obtain from a data dump.\nAn example for using such an article downloading API is provided in the chapter about data mining.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 | Obtaining a set of relevant data sources</span>"
    ]
  },
  {
    "objectID": "obtaining_data/data_mining.html",
    "href": "obtaining_data/data_mining.html",
    "title": "1.2 | Mining data from ChemRxiv",
    "section": "",
    "text": "There are multiple datasets available which are open for data mining.\nTo download full text documents from open access databases the paperscraper tool can be used.\nAs an example, we will download full text articles from ChemRxiv on the topic of ‘buchwald-hartwig coupling’.\n\nfrom paperscraper.get_dumps import chemrxiv\n\n# Download of the ChemRxiv paper dump\nchemrxiv(save_path='chemrxiv_2020-11-10.jsonl')\n\n\nfrom paperscraper.xrxiv.xrxiv_query import XRXivQuery\nfrom paperscraper.pdf import save_pdf_from_dump\nimport pandas as pd\n\ndf = pd.read_json('./chemrxiv_2020-11-10.jsonl', lines=True)\n\n# define keywords for the paper search\nsynthesis = ['synthesis']\nreaction = ['buchwald-hartwig']\n\n# combine keywords using \"AND\" logic, i.e. search for papers that contain both keywords\nquery = [synthesis, reaction]\n\n# start searching for relevant papers in the ChemRxiv dump\nquerier = XRXivQuery('./chemrxiv_2020-11-10.jsonl')\nquerier.search_keywords(query, output_filepath='buchwald-hartwig_coupling_ChemRxiv.jsonl')\n\n# Save PDFs in current folder and name the files by their DOI\nsave_pdf_from_dump('./buchwald-hartwig_coupling_ChemRxiv.jsonl', pdf_path='./PDFs', key_to_save='doi')\n\nFor further steps in the data extraction process annotated data in needed to evaluate the extraction pipeline. For this, one could use an annotation tool like doccano which is shown in the following example.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>1.2 | Mining data from ChemRxiv</span>"
    ]
  },
  {
    "objectID": "constrained_decoding/index.html",
    "href": "constrained_decoding/index.html",
    "title": "Constrained generation to guarantee syntactic correctness",
    "section": "",
    "text": "Defining a data schema\nFor most constrained generation tasks, we need to define a data schema in a programmatic way. The most common way to do so is to use pydantic data classes. Here is an example of a simple data schema for a recipe:\nThis schema can also be extended to include descriptions of different fields or to only allow certain values for specific fields. For example, we could add a field for the number of servings and only allow positive integers.\nIf we want to extract copolymerization reactions a data schema could look like the following.\nWe can now use instructor to “patch” the OpenAI API client to ensure that our output fulfils the schema.\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\n\nclass Monomer(BaseModel):\n    name: str = Field(..., title=\"Name\", description=\"Name of the monomer.\")\n    reactivity_constant: Optional[float] = Field(\n        None,\n        title=\"Reactivity constant\",\n        description=\"Reactivity constant of the monomer. r1 for monomer 1 and r2 for monomer 2. Must be greater or equal 0.\",\n        ge=0,\n    )\n    reactivity_constant_error: Optional[float] = Field(\n        None,\n        title=\"Reactivity constant error\",\n        description=\"Error in the reactivity constant. Often indicated with +/-. Must be greater or equal 0\",\n        ge=0,\n    )\n    q_parameter: Optional[float] = Field(\n        None,\n        title=\"Q parameter\",\n        description=\"Q parameter of the monomer. Q1 for monomer 1 and Q2 for monomer 2. Must be greater or equal 0\",\n        ge=0,\n    )\n    e_parameter: Optional[float] = Field(\n        None,\n        title=\"e parameter\",\n        description=\"e parameter of the monomer. e1 for monomer 1 and e2 for monomer 2.\",\n    )\n\n\nclass CopolymerizationReaction(BaseModel):\n    temperature: Optional[float] = Field(\n        ...,\n        title=\"Temperature\",\n        description=\"Temperature at which the reaction is carried out\",\n    )\n    temperature_unit: Optional[Literal[\"C\", \"K\"]] = Field(\n        ..., title=\"Temperature unit\", description=\"Unit of temperature\"\n    )\n    solvent: Optional[str] = Field(\n        None,\n        title=\"Solvent\",\n        description=\"Solvent used in the reaction. If bulk polymerization was performed, this field should be left empty\",\n    )\n    initiator: Optional[str] = Field(\n        None, title=\"Initiator\", description=\"Initiator used in the reaction\"\n    )\n    monomers: Optional[List[Monomer]] = Field(\n        ...,\n        title=\"Monomers\",\n        description=\"Monomers used in the reaction. Ensure that the reactivity ratios are not confused with other numbers (such as Q and e). The two monomers MUST be used in the same reaction and mentioned in the same context.\",\n        min_items=2,\n        max_items=2,\n    )\n    polymerization_type: Optional[str] = Field(\n        ...,\n        title=\"Polymerization type\",\n        description=\"Type of polymerization (e.g., bulk, solution, suspension, emulsion)\",\n    )\n    determination_method: Optional[str] = Field(\n        ...,\n        title=\"Determination method\",\n        description=\"Method used to determine the reactivity ratios (e.g. Kelen Tudor, Fineman-Ross, Mayo-Lewis).\",\n    )\ndiagram = erd.create(CopolymerizationReaction)\ndiagram.draw(\"diagram.svg\")\nSVG(\"diagram.svg\")\nIn this case, we will use PDF files in the form as images as input for the model. To perform this conversion, we import some utilities.\nfrom pdf2image import convert_from_path\nfrom utils import process_image, get_prompt_vision_model\nThe code below only converts each page of the PDF into an image and then generates dictionary objects in a format that can be used by the OpenAI API.\nfilepath = 'paper01.pdf'\npdf_images = convert_from_path(filepath)\n\nimages_base64 = [process_image(image, 2048, 'images', filepath, j)[0] for j, image in enumerate(pdf_images)]\nimages = get_prompt_vision_model(images_base64=images_base64)\nArmed with the images, we can now use the OpenAI API to extract the text from the images. For this, we just call the API with our prompts and the images.\ncompletion = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    response_model=List[CopolymerizationReaction],\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are a scientific assistant, extracting accurate information about co-polymerization reactions from scientific papers.\nDo not use data that was reproduced from other sources.\nIf you confuse the reactivity ratios with other numbers, you will be penalized.\nMonomer names might be quite similar, if you confuse them, you will be penalized.\nNEVER combine data from different reactions, otherwise you will be penalized.\nIf you are unsure, return no data. Quality is more important than quantity.\n\"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"Extract the data from the paper into the provided data schema. We want an iterable of reaction objects and each reaction will be its own object. You can find each page of the paper as an image below.\nThe relationship between monomers and parameters is typically indicated by subscripts that can be a number or an abbreviation of the monomer.\nNever return data that you are not absolutely sure about! You will be penalized for incorrect data.\"\"\",\n        },\n        {\"role\": \"user\", \"content\": [*images]},\n    ],\n    temperature=0,\n)\ncompletion\n\n[CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='carbon tetrachloride', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.54, reactivity_constant_error=0.01, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.06, reactivity_constant_error=0.03, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='chloroform', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.51, reactivity_constant_error=0.01, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.08, reactivity_constant_error=0.03, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='acetone', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.43, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.65, reactivity_constant_error=0.02, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='1,4-dioxane', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.41, reactivity_constant_error=0.02, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.59, reactivity_constant_error=0.05, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='acetonitrile', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.12, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.29, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos')]",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Constrained generation to guarantee syntactic correctness</span>"
    ]
  },
  {
    "objectID": "constrained_decoding/index.html#defining-a-data-schema",
    "href": "constrained_decoding/index.html#defining-a-data-schema",
    "title": "Constrained generation to guarantee syntactic correctness",
    "section": "",
    "text": "from pydantic import BaseModel\n\nclass Recipe(BaseModel):\n    title: str\n    ingredients: List[str]\n    instructions: List[str]\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, List\n\nclass Recipe(BaseModel):\n    title: str\n    ingredients: List[str]\n    instructions: List[str]\n    servings: int = Field(..., gt=0, description=\"The number of servings for this recipe\")\n    rating: Literal[\"easy\", \"medium\", \"hard\"] = Field(\"easy\", description=\"The difficulty level of this recipe\")",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Constrained generation to guarantee syntactic correctness</span>"
    ]
  },
  {
    "objectID": "ocr/nougat/nougat.html",
    "href": "ocr/nougat/nougat.html",
    "title": "OCR with Nougat",
    "section": "",
    "text": "Cleaning the data\nFor certain applications it might be necessary to clean the data before using it for other downstream tasks.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>OCR with Nougat</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative structured data extraction using LLMs",
    "section": "",
    "text": "About this book\nStructured data is at the heart of machine learning. LLMs offer a convenient way to generate structured data based on unstructured inputs. This book gives hands-on examples of the different steps in the extraction workflow using LLMs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Generative structured data extraction using LLMs</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html",
    "href": "finetune/choosing_paradigm.html",
    "title": "4 | Choosing the learning paradigm",
    "section": "",
    "text": "First steps\nChoosing the learning paradigm should begin by trying some leading general-purpose LLM. For this practical case, the first model to test is the recent Llama-3 8B model with zero and one-shot prompts.\nWe will start by importing all the packages needed.\nimport json \nfrom dotenv import load_dotenv\n\nimport torch\nfrom datasets import (\n    load_dataset,\n    Dataset,\n)\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n)\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom peft import (\n    LoraConfig,\n)\nfrom trl import (\n    SFTTrainer,\n    DataCollatorForCompletionOnlyLM,\n)\nfrom evaluate import load\nimport litellm\nfrom litellm import completion\nfrom litellm.caching import Cache\nfrom statistics import mean\nimport numpy as np \nimport matplotlib.pyplot as plt\nTo continue, we will allow LiteLLM to cache requests made to LLM-APIs. Additionally, we will import all environment variables.\nlitellm.cache = Cache()\nload_dotenv(\".env\", override=True)",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#first-steps",
    "href": "finetune/choosing_paradigm.html#first-steps",
    "title": "4 | Choosing the learning paradigm",
    "section": "",
    "text": "Note that using the environment variables is the safest way of keeping personal API keys secret.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#first-model-and-dataset",
    "href": "finetune/choosing_paradigm.html#first-model-and-dataset",
    "title": "4 | Choosing the learning paradigm",
    "section": "First model and dataset",
    "text": "First model and dataset\nAs starting model, we will try the Llama-3 8B model. We will call this model through the Groq API, which allows performing fast inference with several open-source models.\n\n\nGroq provides some of the most popular open-source models, such as Llama or Mixtral models, with a high inference speed. To use the Groq API, the .env file must also contain the GROQ_API_KEY.\n\nbase_model = \"groq/llama3-8b-8192\"\n\nThe dataset used in this tutorial is the one used in Ai et al.’s (Ai et al. 2024) recent work, which contains data about chemical reactions. The dataset contains 100K reaction procedure—ORD JSON pairs.\n\n\nORD stands for Open Reaction Database, a comprehensive data structure specially designed to describe all the elements involved in chemical reactions.\n\n\n\n\n\n\nDownload data\n\n\n\n\n\nThe best way to obtain the data is to install the GitHub repository of the Ai et al. work and take it from there. To do so, run the following commands:\n!git clone https://github.com/qai222/LLM_organic_synthesis.git\n!cp LLM_organic_synthesis/workplace_data/datasets/USPTO-n100k-t2048_exp1.7z .\n!7za x USPTO-n100k-t2048_exp1.7z\n!cp USPTO-n100k-t2048_exp1/*.json .\n!rm -rf USPTO-n100k-t2048_exp1/ USPTO-n100k-t2048_exp1.7z LLM_organic_synthesis\nThis will leave four .json files in your current directory that contain all the data used here.\n\n\n\n\ntest_ds_path = \"test.json\"\ntest_dataset = load_dataset(\"json\", data_files=test_ds_path, split=\"train\")\ntest_dataset = test_dataset.shuffle(seed=42).select(range(100))\ntest_dataset\n\nDataset({\n    features: ['output', 'instruction'],\n    num_rows: 100\n})\n\n\n\n\nNote that we only selected 100 samples from the test set. This will be enough for the demo shown here.\n\ntest_dataset[0]\n\n{'output': '{\"inputs\": {\"m1\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"4-nitrobenzyl (1R,3R,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-oxo-1-carbapenam-3-carboxylate\"}], \"amount\": {\"mass\": {\"value\": 743.0, \"units\": \"MILLIGRAM\"}}, \"reaction_role\": \"REACTANT\"}]}, \"m2\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"2-(tri-n-butylstannyl)-7-trifluoromethylthioimidazo[5,1-b]thiazole\"}], \"amount\": {\"mass\": {\"value\": 1.06, \"units\": \"GRAM\"}}, \"reaction_role\": \"REACTANT\"}]}}, \"conditions\": {\"conditions_are_dynamic\": true}, \"outcomes\": [{\"products\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"4-nitrobenzyl (1S,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-(7-trifluoromethylthioimidazo[5,1-b]thiazol-2-yl)-1-carbapen-2-em-3-carboxylate\"}], \"measurements\": [{\"type\": \"AMOUNT\", \"details\": \"MASS\", \"amount\": {\"mass\": {\"value\": 172.0, \"units\": \"MILLIGRAM\"}}}], \"reaction_role\": \"PRODUCT\"}]}]}',\n 'instruction': 'Below is a description of an organic reaction. Extract information from it to an ORD JSON record.\\n\\n### Procedure:\\nThe procedure of Example 1a) was repeated, except that 743 mg of 4-nitrobenzyl (1R,3R,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-oxo-1-carbapenam-3-carboxylate and 1.06 g 2-(tri-n-butylstannyl)-7-trifluoromethylthioimidazo[5,1-b]thiazole were used as the starting compounds. Thus, 172 mg of 4-nitrobenzyl (1S,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-(7-trifluoromethylthioimidazo[5,1-b]thiazol-2-yl)-1-carbapen-2-em-3-carboxylate was prepared.\\n\\n### ORD JSON:\\n'}\n\n\n\n\nNote that the output comes in JSON format. For this simple example, we will not constrain the output to JSON format. However, to ensure good results, this constraining must be done. The chapter [“Constrained decoding and enforcing valid outputs”] (./file-you-link-to.qmd) provides explanations and examples about the options and how to constrain the output.\nThis dataset is very big. Therefore, we will only take 100 samples from the test set used in the article mentioned above for our test set.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#prompt-and-inference",
    "href": "finetune/choosing_paradigm.html#prompt-and-inference",
    "title": "4 | Choosing the learning paradigm",
    "section": "Prompt and Inference",
    "text": "Prompt and Inference\nWe define a simple prompt template. The prompt contains a simple system part (named PREFIX) where the role and task of the model are defined, as well as the example used only for the one-shot prompt. Additionally, the prompt has a user prompt where the reaction instruction will be provided.\n\nPREFIX = \"\"\"You are a helpful scientific assistant. Your task is to extract information about organic reactions. {shot}\"\"\"\nSUFFIX = \"\"\"\\n\\n{sample}\\n\\n\"\"\"\nSHOT = \"\"\"\nOne example is provided to you to show how to perform the task:\n\n### Procedure:\\nA suspension of 8 g of the product of Example 7 and 0.4 g of DABCO in 90 ml of xylenes were heated under N2 at 130\\u00b0-135\\u00b0 C. while 1.8 ml of phosgene was added portionwise at a rate to maintain a reflux temperature of about 130\\u00b0-135\\u00b0 C. The mixture was refluxed an additional two hours, cooled under N2 to room temperature, filtered, and the filtrate was concentrated in vacuo to yield 6.9 g of the subject compound as a crude oil.\\n\\n\n### ORD JSON:\\n{\\\"inputs\\\": {\\\"m1_m2_m4\\\": {\\\"components\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"product\\\"}], \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 8.0, \\\"units\\\": \\\"GRAM\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}, {\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"DABCO\\\"}], \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 0.4, \\\"units\\\": \\\"GRAM\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}, {\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"xylenes\\\"}], \\\"amount\\\": {\\\"volume\\\": {\\\"value\\\": 90.0, \\\"units\\\": \\\"MILLILITER\\\"}}, \\\"reaction_role\\\": \\\"SOLVENT\\\"}]}, \\\"m3\\\": {\\\"components\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"phosgene\\\"}], \\\"amount\\\": {\\\"volume\\\": {\\\"value\\\": 1.8, \\\"units\\\": \\\"MILLILITER\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}]}}, \\\"conditions\\\": {\\\"temperature\\\": {\\\"control\\\": {\\\"type\\\": \\\"AMBIENT\\\"}}, \\\"conditions_are_dynamic\\\": true}, \\\"workups\\\": [{\\\"type\\\": \\\"ADDITION\\\", \\\"details\\\": \\\"was added portionwise at a rate\\\"}, {\\\"type\\\": \\\"TEMPERATURE\\\", \\\"details\\\": \\\"to maintain a reflux temperature of about 130\\\\u00b0-135\\\\u00b0 C\\\"}, {\\\"type\\\": \\\"TEMPERATURE\\\", \\\"details\\\": \\\"The mixture was refluxed an additional two hours\\\", \\\"duration\\\": {\\\"value\\\": 2.0, \\\"units\\\": \\\"HOUR\\\"}}, {\\\"type\\\": \\\"FILTRATION\\\", \\\"details\\\": \\\"filtered\\\"}, {\\\"type\\\": \\\"CONCENTRATION\\\", \\\"details\\\": \\\"the filtrate was concentrated in vacuo\\\"}], \\\"outcomes\\\": [{\\\"products\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"subject compound\\\"}], \\\"measurements\\\": [{\\\"type\\\": \\\"AMOUNT\\\", \\\"details\\\": \\\"MASS\\\", \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 6.9, \\\"units\\\": \\\"GRAM\\\"}}}], \\\"reaction_role\\\": \\\"PRODUCT\\\"}]}]}\n\\n\n\"\"\"\n\n\n\nPREFIX is supposed to be the content of the system prompt. SUFFIX is the user prompt. SHOT is the 1-shot prompt that will be added to the system prompt when used.\nTo continue, we loop all over the dataset two times, one for each type of prompt (zero- and one-shot). For each dataset sample, we format the prompt to include the procedure-output schema pairs using the template defined in the previous cell. In addition, we also predict using the model and store those predictions for future evaluation.\n\nshots = [\"0-shot\", \"1-shot\"]\nresults_llama = {}\n\n# Start by looping over the shots\nfor s in shots:\n    predictions = []\n    references = []\n\n# Loop over all the samples of the dataset\n    for t in test_dataset:\n        instruction = t[\"instruction\"]\n        output = t[\"output\"]\n        # Format the prompt\n        if s == \"0-shot\":\n            shot = \"\"\n        else:\n            shot = SHOT\n        system = PREFIX.format(shot=shot)\n        user = SUFFIX.format(sample=instruction)\n        prompt = [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user},\n        ]\n        # Do the completion using Groq API through LiteLLM\n        pred = (\n            completion(\n                model=base_model,\n                messages=prompt,\n                caching=True,\n                temperature=0,\n            )\n            .choices[0]\n            .message.content\n        )\n        # Save the predictions and the references for later evaluation\n        references.append(output)\n        predictions.append(pred)\n\n    results_llama[s] = {\n        \"predictions\": predictions,\n        \"references\": references,\n    }\n\n\n\nThe beauty of LiteLLM is that it allows obtaining completions from models by different providers using the OpenAI prompt completions’ schema for all providers:\n{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": user_message}\nAfter generating the predictions, it’s essential to evaluate them. We will initially use the BERTScore for a simple evaluation, as it provides precision, recall, and F\\(_1\\) scores based on similarity measures. However, for a complex schema like the one we are predicting, more robust evaluation methods should be utilized. (REF EVALUATION CHAPTER)\n\n\n\n\n\n\nNotes about BERTScore\n\n\n\n\n\nBERTScore (zhang2020bertscore?) is an evaluation method that proceeds by calculating the similarity of the candidate text with the reference. This similarity is calculated as a sum of cosine similarities token by token. To produce the embeddings this similarity calculation, in the original article, they used the BERT model’s embeddings. However, for our case we will be using the embeddings from the DistilBERT model (sanh2020distilbert?) which achieves 97% of the original BERT model language understanding while only being 40% the size of BERT original model.\n\n\n\n\nbertscore = load(\"bertscore\")\nshots = ['0-shot', '1-shot']\n\n# Start by looping over the shots\nfor s in shots:\n    predictions = results_llama[s][\"predictions\"]\n    references = results_llama[s][\"references\"]\n\n    results_ = bertscore.compute(\n        predictions=predictions,\n        references=references,\n        model_type=\"distilbert-base-uncased\",\n    )\n\n    results_llama[s].update(\n        {\n            \"precision\": mean(results_[\"precision\"]),\n            \"recall\": mean(results_[\"recall\"]),\n            \"f1_scores\": mean(results_[\"f1\"]),\n        }\n    )\n\n\n\nResults for the 0-shot prompt\n    Precision: 0.865\n    Recall: 0.8918\n    F1-Score: 0.8781\n\nResults for the 1-shot prompt\n    Precision: 0.9392\n    Recall: 0.9553\n    F1-Score: 0.9471\n\n\n\nThe results are excellent, especially with the one-shot prompt. However, we are going to try a different model, a closed-source model, to compare.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#another-model-closed-source-this-time",
    "href": "finetune/choosing_paradigm.html#another-model-closed-source-this-time",
    "title": "4 | Choosing the learning paradigm",
    "section": "Another model, closed-source this time",
    "text": "Another model, closed-source this time\nThe second model we will use is the newer OpenAI, GPT-4o. Doing this allows us to compare open- and closed-source models.\nThe procedure and code are exactly the same as for the previous case; the only difference is to define a different model.\n\nbase_model = \"gpt-4o\"\n\n\n\nOpenAI models are also supported by the LiteLLM package.\nAnd we obtain the completions using both prompts for all the test samples.\n\nresults_openai = {}\nshots = [\"0-shot\", \"1-shot\"]\n\n# Start by looping over the shots\nfor s in shots:\n    predictions = []\n    references = []\n\n# Loop over all the samples of the dataset\n    for t in test_dataset:\n        instruction = t[\"instruction\"]\n        output = t[\"output\"]\n        # Format the prompt following OpenAI's prompting guidelines\n        if s == \"0-shot\":\n            shot = \"\"\n        else:\n            shot = SHOT\n        system = PREFIX.format(shot=shot)\n        user = SUFFIX.format(sample=instruction)\n        prompt = [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user},\n        ]\n        # Do the completion using Groq API through LiteLLM\n        pred = (\n            completion(\n                model=base_model,\n                messages=prompt,\n                caching=True,\n                temperature=0,\n            )\n            .choices[0]\n            .message.content\n        )\n        # Remove some residual stuff in the json output by the model.\n        if \"```json\" in pred:\n            pred = pred.replace(\"```json\\n\", \"\")\n            pred = pred.replace(\"```\", \"\")\n            \n        # Save the predictions and the references for later evaluation\n        references.append(output)\n        predictions.append(pred)\n\n    results_openai[s] = {\n        \"predictions\": predictions,\n        \"references\": references,\n    }\n\nFinally, we evaluate again using BERTScore.\n\nfor s in shots:\n    predictions = results_openai[s][\"predictions\"]\n    references = results_openai[s][\"references\"]\n\n    results_ = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n\n    results_openai[s].update({\n        \"precision\": mean(results_[\"precision\"]),\n        \"recall\": mean(results_[\"recall\"]),\n        \"f1_scores\": mean(results_[\"f1\"]),\n    })\n\n\n\nResults for the 0-shot prompt\n    Precision: 0.8949\n    Recall: 0.9093\n    F1-Score: 0.9019\n\nResults for the 1-shot prompt\n    Precision: 0.9545\n    Recall: 0.9619\n    F1-Score: 0.9581\n\n\n\nThe results with this GPT-4o model are excellent, improving slightly on the ones obtained with the Llama-3 8B base model. However, we are going to try to improve these results further by fine-tuning the Llama-3 8B model.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#fine-tuning",
    "href": "finetune/choosing_paradigm.html#fine-tuning",
    "title": "4 | Choosing the learning paradigm",
    "section": "Fine-tuning",
    "text": "Fine-tuning\nAs the final step, we will fine-tune the Llama-3 8B using data similar to the one we used above.\nWe will use packages built by HuggingFace to do the fine-tuning.\nFirst, we define the base model we will use and the path of the dataset.\n\n# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n\n\n\n\n\n\nCaution\n\n\n\nIt is important to include the HF_token in the .env file. When we created this notebook, the model we will fine-tune (Llama3-8B) was only available after an access request.\n\n\nThe next step is to load the dataset for the fine-tuning. For that, similar to the testing of the previous models, we will use the dataset used by Ai et al., but for this case, we will use their train dataset. Since this is a quick demonstration, we will only take 5000 samples.\n\ndataset = load_dataset(\"json\", data_files=\"train.json\", split=\"train\")\ndataset = dataset.shuffle(seed=42).select(range(5000)) # Only use 5000 samples for quick demo\ndataset = dataset.train_test_split(test_size=0.1, seed=42) # We define 90-10 % training-evaluation splits.\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['output', 'instruction'],\n        num_rows: 4500\n    })\n    test: Dataset({\n        features: ['output', 'instruction'],\n        num_rows: 500\n    })\n})\n\n\nThen, we define the method to fine-tune the model. For this fine-tuning, we will use the popular QLoRA method. QLoRA (Dettmers et al. 2023) is an efficient approach that reduces memory usage during fine-tuning while preserving full fine-tuning task performance.\n\n\n\n\n\n\nNotes about QLoRA configuration\n\n\n\n\n\n\nload_in_4bit=True: loads the model using the 4-bit quantization.\nbnb_4bit_quant_type=\"nf4\": quantizes following the nf4 method.(Dettmers et al. 2022)\nbnb_4bit_use_double_quant=True: activate nested quantization for 4-bit base models.\nbnb_4bit_compute_dtype=torch.bfloat16: Compute dtype for 4-bit base models.\n\n\n\n\n\n# QLoRA configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\", # fp4 or nf4\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n\n\n\n\n\n\nNotes about LoRA configuration\n\n\n\n\n\n\nr: The rank of the updated matrices, expressed as integer, meaning that the adaptor that is build in top of the model to improved will be made by matrices of rank 32. Lower rank results in smaller update matrices with fewer trainable parameters that can not be enough to capture the diverse data during the training. On the other hand, higher ranks may lead to overfitting. This rank is a hyperparameter that needs to be optimized.\n\n\n\n\n\n\n\nFigure 7.1: Image illustrating LoRA rank. Note that lower is the rank, lower is the dimension of the AB matrix multiplication, thus reducing the cost.\n\n\n\n\nlora_alpha: LoRA scaling factor. It changes how the adaptation layer’s weights affect the base model’s.\nlora_dropout: Dropout is a regularization technique where a proportion of neurons (or parameters) are randomly “dropped out” or turned off during training to prevent overfitting.\nbias: Specifies if the bias parameters should be trained. Can be ‘none’, ‘all’ or ‘lora_only’.\ntask_type: Task to perform, “Causal LM”: Causal language modeling.\n\n\n\n\n\npeft_config = LoraConfig(\n    r=32,\n    lora_alpha=64, \n    lora_dropout=0.1,\n    bias=\"none\", \n    task_type=\"CAUSAL_LM\", \n)\n\nBefore training, we define the tokenizer and the model for fine-tuning, set the training arguments, and initialize the trainer.\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model) # Define the tokenizer\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\" # Where the \"pad_token\" is placed\n\n# Model config\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model, # Model that we are going to fine-tune\n    quantization_config=bnb_config, # QLoRA config defined above\n    device_map=\"auto\", # Where the model is trained, set device_map=\"auto\" loads a model onto available GPUs first.\n)\n\n\n\nThe pad_token is a special token used to make arrays of tokens the same size for batching purpose. The typical is to use the eos_token which is a special token representing the end of a sentence.\n\n\n\n\n\n\nNotes about the training arguments\n\n\n\n\n\n\nlearning_rate: the learning rate is a hyperparameter that sets how the training algorithm updates the values of the weights.\nBatch size: it is the number of samples used in one forward and backward pass through the network. Ideally, we would like to increase this number so the fine-tuning will be faster. The problem is that for higher batch number, more GPU memory is needed. For example, for training the model used in this demonstration using the exact same configuration but with a default token length (1024 tokens), with 40 GB VRAM GPU, the maximum batch number is 2. Using 80 GB VRAM GPU, the batch size can be increased to 4.\n\nper_device_train_batch_size: batch size for the training.\nper_device_eval_batch_size: batch size for the evaluation.\n\ngradient_accumulation_steps: number of accumulated gradients over each batch. Gradient accumulation is a technique that simulates a larger batch size and it is very related to the batch size, since it also allows reducing the computation time. However, contrary to the batch size, during the gradient accumulation the weights of the model are not updated during each forward and backward pass, but gradients are accumulated from multiple small batches before performing the update. Thus, setting a higher gradient_accumulation_steps can help to accelerate the training when increasing the batch size is not possible due to VRAM impediments.\noptim: optimizer used. The main role of the optimizer is to minimize the loss function. The paged_adamw_32bit is the well-known AdamW optimizer. AdamW optimization is a stochastic gradient descent method.(Loshchilov and Hutter 2019)\nnum_train_epochs: number of times that the model goes through each sample during the training. A larger number might lead to the best training results or to overfitting. A lower number might give a model that does not work as expected at all.\nfp16 and bf16: these parameters help to achieve mixed precision training, which is a technique that aims to optimize the computational efficiency of training models by utilizing lower-precision numerical formats for certain variables. However, the choice of both parameters depends on the architecture of the GPUs that are being used during the training.\nlogging_steps: when the logging is done.\nevaluation_strategy: the evaluation strategy to adopt during training. The most used is ‘steps’ meaning that the evaluation is done after a certain number of training steps.\neval_steps: define in which steps the evaluation is done.\nmax_grad_norm: maximum gradient norm (for gradient clipping). Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and using the clipped gradients to update the weights.\nwarmup_steps: number of steps used for a linear warm-up from 0 to learning_rate. The warm-up helps to stabilize the optimization process and prevent divergence.\nwarmup_ratio: ratio of total training steps used for the linear warm-up.\ngroup_by_length: whether to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding.\nlr_scheduler_type: describes the decay of the learning rate during the training.\noutput_dir: directory to safe the report of the training process.\nsave_strategy: what we want to save during the training. Set it to “no” to only safe the final model.\n\n\n\n\n\n\n\nFigure 7.2: Shape of the “cosine” lr_scheduler_type option.\n\n\n\n\n\n\n\n# Define the different hyperparameters and arguments for the fine-tuning\ntraining_arguments = TrainingArguments(\n    learning_rate=6e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=10,\n    fp16=False,\n    bf16=True, #bf16 to True with an A100, False otherwise\n    logging_steps=1, # Logging is done every step.\n    evaluation_strategy=\"steps\", \n    eval_steps=0.01, \n    max_grad_norm=0.3,\n    warmup_steps=100, \n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\", \n    output_dir=\"./results/\", \n    save_strategy='no', \n)\n\n\nresponse_template = \" ### Answer:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\n\nData collators are objects that will form a batch by using a list of dataset elements as input. There is one data collator for each task, here we use the one for completion-only.\n\n\n\n\n\n\nNotes about the Completion-only training\n\n\n\nThe completion-only training instead of training the model on the whole input (prompt + answer) make the training more efficient by training only the model on completion. This can sometimes increase the performance, especially for situations like ours in which we want to use the model only for completions, and not to generate more instructions.\n\n\n\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example[\"instruction\"])):\n        text = f\"### Question: {example[\"instruction\"][i]}\\n ### Answer: {example[\"output\"][i]}\"\n        output_texts.append(text)\n    return output_texts\n\n\n\nThe formatting function is intended for cases where the prompt is constructed from more than one feature of the dataset. Using the formatting functions allows us to join them optimally.\n\ntrainer = SFTTrainer(\n    model=model, # Model to fine-tune\n    max_seq_length=2048, # Max number of tokens of the completion\n    args=training_arguments, # Training arguments to use \n    train_dataset=dataset[\"train\"], # Set of the dataset used for the training\n    eval_dataset=dataset[\"test\"], # Set of the dataset used for the evaluations\n    peft_config=peft_config, # Configuration and PEFT method to use\n    tokenizer=tokenizer, # Tokenizer used \n    packing=False,\n    formatting_func=formatting_prompts_func, # Prompt formatting function\n    data_collator=collator, \n)\n\n\n\nSome of the prompts used are bigger than the default value of 1024 tokens, so we must set a max_seq_length bigger than that.\n\nWhen packing is set to True during training, multiple short examples fill in the input sequence length instead of padding them to increase training efficiency. However, when using a collator, it must be False.\n\n\n\n\n\n\n\nCaution\n\n\n\nWe set a very small eval_steps variable so the training has a lot of evals, to obtain more visual loss curves. Ideally, that much evals steps are not needed and will make the training process slower.\n\n\nAnd finally when everything is ready we train the model.\n\ntrainer.train()\n\n\n\n    \n      \n      \n      [2810/2810 6:09:59, Epoch 9/10]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n29\n0.630700\n0.593230\n\n\n58\n0.280600\n0.287035\n\n\n87\n0.094700\n0.088160\n\n\n116\n0.075500\n0.072126\n\n\n145\n0.071100\n0.066134\n\n\n174\n0.053100\n0.055799\n\n\n203\n0.065700\n0.054912\n\n\n232\n0.045500\n0.048931\n\n\n261\n0.047300\n0.050089\n\n\n290\n0.041400\n0.049855\n\n\n319\n0.044300\n0.043511\n\n\n348\n0.041200\n0.042149\n\n\n377\n0.042800\n0.042795\n\n\n406\n0.038100\n0.039256\n\n\n435\n0.045000\n0.041538\n\n\n464\n0.039600\n0.037373\n\n\n493\n0.045400\n0.038947\n\n\n522\n0.036700\n0.036142\n\n\n551\n0.037700\n0.035618\n\n\n580\n0.020200\n0.034974\n\n\n609\n0.023400\n0.035457\n\n\n638\n0.023100\n0.033618\n\n\n667\n0.034600\n0.035628\n\n\n696\n0.031500\n0.032545\n\n\n725\n0.025500\n0.033716\n\n\n754\n0.027200\n0.032997\n\n\n783\n0.035700\n0.032624\n\n\n812\n0.023700\n0.033319\n\n\n841\n0.030000\n0.032736\n\n\n870\n0.023900\n0.031512\n\n\n899\n0.026400\n0.031785\n\n\n928\n0.017500\n0.030995\n\n\n957\n0.023000\n0.031565\n\n\n986\n0.024500\n0.030332\n\n\n1015\n0.017600\n0.030359\n\n\n1044\n0.020000\n0.034667\n\n\n1073\n0.026300\n0.029186\n\n\n1102\n0.025200\n0.030255\n\n\n1131\n0.021600\n0.029377\n\n\n1160\n0.017400\n0.028646\n\n\n1189\n0.023600\n0.029402\n\n\n1218\n0.026700\n0.028909\n\n\n1247\n0.015600\n0.028068\n\n\n1276\n0.033300\n0.029490\n\n\n1305\n0.020000\n0.028139\n\n\n1334\n0.029600\n0.029012\n\n\n1363\n0.022500\n0.027676\n\n\n1392\n0.014100\n0.028040\n\n\n1421\n0.016300\n0.028978\n\n\n1450\n0.015900\n0.027864\n\n\n1479\n0.017700\n0.027962\n\n\n1508\n0.024100\n0.028795\n\n\n1537\n0.014900\n0.027778\n\n\n1566\n0.020100\n0.028307\n\n\n1595\n0.016400\n0.027065\n\n\n1624\n0.022100\n0.027976\n\n\n1653\n0.009900\n0.027239\n\n\n1682\n0.017600\n0.026706\n\n\n1711\n0.020000\n0.027776\n\n\n1740\n0.020400\n0.028023\n\n\n1769\n0.014200\n0.027051\n\n\n1798\n0.020500\n0.028300\n\n\n1827\n0.012600\n0.027237\n\n\n1856\n0.019300\n0.027134\n\n\n1885\n0.014500\n0.027291\n\n\n1914\n0.014800\n0.027198\n\n\n1943\n0.018400\n0.027487\n\n\n1972\n0.016500\n0.026561\n\n\n2001\n0.013400\n0.027307\n\n\n2030\n0.018600\n0.028109\n\n\n2059\n0.017900\n0.028044\n\n\n2088\n0.019900\n0.027581\n\n\n2117\n0.007100\n0.027354\n\n\n2146\n0.016400\n0.027352\n\n\n2175\n0.020500\n0.027349\n\n\n2204\n0.013000\n0.027181\n\n\n2233\n0.017700\n0.027262\n\n\n2262\n0.019200\n0.027305\n\n\n2291\n0.014000\n0.027452\n\n\n2320\n0.014300\n0.027622\n\n\n2349\n0.004600\n0.027471\n\n\n2378\n0.010000\n0.027483\n\n\n2407\n0.024700\n0.027695\n\n\n2436\n0.011400\n0.027572\n\n\n2465\n0.012000\n0.027545\n\n\n2494\n0.007100\n0.027586\n\n\n2523\n0.010500\n0.027584\n\n\n2552\n0.015100\n0.027578\n\n\n2581\n0.004900\n0.027581\n\n\n2610\n0.016500\n0.027570\n\n\n2639\n0.016800\n0.027609\n\n\n2668\n0.016900\n0.027610\n\n\n2697\n0.019100\n0.027594\n\n\n2726\n0.012600\n0.027575\n\n\n2755\n0.016100\n0.027580\n\n\n2784\n0.022600\n0.027586\n\n\n\n\n\n\n\nTrainOutput(global_step=2810, training_loss=0.03654618244585034, metrics={'train_runtime': 22218.9233, 'train_samples_per_second': 2.025, 'train_steps_per_second': 0.126, 'total_flos': 1.8010605645245645e+18, 'train_loss': 0.03654618244585034, 'epoch': 9.991111111111111})\n\n\nTo better visualize how the fine-tuning went, the best option is to plot the loss curves for the training and for the evaluation. The ideal loss curve depicts the model’s loss values over time. At first, the loss is high, but it gradually declines, signifying that the model’s performance is improving.\n\n\n\n\n\n\nFigure 7.3: Training-loss curve\n\n\n\n\n\n\n\n\n\nFigure 7.4: Evaluation-loss curve\n\n\n\nThe loss curves produced during the fine-tuning of our model are not far from the ideal behavior meaning that the training proceeded correctly.\n\n\nThe loss curves shown here were automatically created by reporting the training to WandB. This is a helpful possibility to easily obtain the loss curves for the fine-tuning.\nThe easiest way to evaluate the fine-tuned model and perform inference is to use the trained model directly. To do that, we have to define a pipeline for text generation, do the inference using that pipeline, and evaluate similarly as for the previous models.\n\n# Define the pipeline that will do the inference\nsft_pipe = pipeline(\n    \"text-generation\",\n    do_sample=False, # This allows to set Temperature to 0 (or None for this case)\n    temperature=None,\n    model=trainer.model, # We do the inference with the trained model.\n    tokenizer=tokenizer,\n)\n\n\n\nThe temperature defines the degrees of freedom that are allowed to the model when it predicts the next word. For data extraction, the best value is 0 because the model do not need to make up the data, only to extract it from the corresponding text. This is because at temperature equal to 0, the model is going to pick always the most probable token.\n\n# Create the 0 and 1-shot prompts.\nresults_sft = {}\nprompts_ = {}\nshots = [\"0-shot\", \"1-shot\"]\n\n# Start by looping over the shots\nfor s in shots:\n    references = []\n    prompts = []\n    \n    # Loop over all the samples of the dataset\n    for t in test_dataset:\n        instruction = t[\"instruction\"]\n        output = t[\"output\"]\n        references.append(output)\n        if s == \"0-shot\":\n            shot = ''\n        else:\n            shot = SHOT\n        # Format the prompt\n        system = PREFIX.format(shot=shot)\n        user = SUFFIX.format(sample=instruction)\n        prompt = system + user\n        prompts.append(prompt)\n\n    # Save the prompts and the references.\n    prompts_[s] = {\n        \"prompts\": prompts,\n    }\n    results_sft[s] = {\n        \"references\": references,\n    }\n\n\n# Do the inference using batching.\nfor s in shots:\n    # Create a tmp dataset to make easier the batching\n    ds = Dataset.from_dict(prompts_[s])\n    predictions_sft = []\n    # Inference time!\n    with torch.cuda.amp.autocast():\n        for out in sft_pipe(KeyDataset(ds, \"prompts\"), batch_size=16):\n            # Clean the output.\n            for i, sample in enumerate(out):\n                sample[\"generated_text\"] = sample[\"generated_text\"].replace(prompts_[s][\"prompts\"][i], \"\")\n                predictions_sft.append(sample[\"generated_text\"])\n            \n    # Save the results.\n    results_sft[s].update({\n        \"predictions\": predictions_sft,\n    })\n\n\n\nNote that we do not use LiteLLM for this particular case. This is because this package does not yet support completion-only tasks with HuggingFace models. Thus, the prompt is not written according to the OpenAI completions guide.\n\nWe perform the inference using the torch.cuda.amp.autocast function. This allows us to avoid errors related with different data types.\n\nNote that after completion, we replace the prompt with an empty string. We do this because the model was always instructed to complete the instructions with the prompt at the beginning. Above, for the case where we used this same Llama model, this was not done because the API already provided the completion without the prompt.\n\n\nThe limiting factor for the inference when batching is the GPU memory. This is because during inference the GPU will contain not only the model but an amount of prompts equals to the batch size.\nFinally, we calculate the metrics to evaluate this last model’s results.\n\nfor s in shots:\n    predictions_sft = results_sft[s][\"predictions\"]\n    references = results_sft[s][\"references\"]\n\n    results = bertscore.compute(predictions=predictions_sft, references=references, model_type=\"distilbert-base-uncased\")\n\n    results_sft[s].update({\n        \"precision\": mean(results[\"precision\"]),\n        \"recall\": mean(results[\"recall\"]),\n        \"f1_scores\": mean(results[\"f1\"]),\n    })\n\n\n\nResults for the 0-shot prompt\n    Precision: 0.8383\n    Recall: 0.906\n    F1-Score: 0.8704\n\nResults for the 1-shot prompt\n    Precision: 0.8666\n    Recall: 0.9126\n    F1-Score: 0.8889\n\n\n\nThe results using the 0-shot prompt are similar as the other models.\nOn the other hand, for this fine-tuned model, the 1-shot results do not show an improvement as big as for the other models. This is because when fine-tuning is done, the model gets used to a very robust prompt-completion format, that for the case of the 1-shot prompt is broken resulting in worse results than expected.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#visualization-of-the-results",
    "href": "finetune/choosing_paradigm.html#visualization-of-the-results",
    "title": "4 | Choosing the learning paradigm",
    "section": "Visualization of the results",
    "text": "Visualization of the results\nTo study the results more graphically, we can plot all the results in several bar plots.\n\n\nCode\n# set width of bar\nbarWidth = 0.2\nfig = plt.subplots(figsize=(20, 10))\n\nplt_models = [\"Llama3-8B\", \"Llama3-8B Fine-tuned\", \"GPT-4o\"]\nplt_metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\nplt_data = {}\nfor index, model in enumerate(plt_models):\n    plt_data[model] = metrics_0_shot[index]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(metrics_0_shot[0]))\nbr2 = [x + barWidth for x in br1]\nbr3 = [x + barWidth for x in br2]\n\n# Make the plot\nplt.bar(\n    br1,\n    metrics_0_shot[0],\n    color=\"b\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[0],\n)\nplt.bar(\n    br2,\n    metrics_0_shot[1],\n    color=\"skyblue\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[1],\n)\nplt.bar(\n    br3,\n    metrics_0_shot[2],\n    color=\"mediumseagreen\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[2],\n)\n\n# Adding Xticks\nplt.xlabel(\"Metric\", fontweight=\"bold\", fontsize=15)\nplt.ylabel(\"Results\", fontweight=\"bold\", fontsize=15)\nplt.xticks(\n    [r + barWidth for r in range(len(metrics_0_shot[0]))], plt_metrics, fontsize=12\n)\nplt.ylim(0, 1)\nplt.title(\"Different metrics with 0 shot prompt for the models\", fontsize=20)\n\nplt.legend()\nplt.savefig(\"bars0.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFor the 0-shot prompt is possible to see that the best results for all the metrics are the ones obtained when using the closed-source model that is presumably the bigger one of the three. Attending to both Llama3-8B models the results are very similar. The fact that all the results are that similar between them is probably because of the evaluation used.\n\n\nCode\n# set width of bar\nbarWidth = 0.2\nfig = plt.subplots(figsize=(20, 10))\n\nplt_data = {}\nfor index, model in enumerate(plt_models):\n    plt_data[model] = metrics_1_shot[index]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(metrics_1_shot[0]))\nbr2 = [x + barWidth for x in br1]\nbr3 = [x + barWidth for x in br2]\n\n# Make the plot\nplt.bar(\n    br1,\n    metrics_1_shot[0],\n    color=\"b\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[0],\n)\nplt.bar(\n    br2,\n    metrics_1_shot[1],\n    color=\"skyblue\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[1],\n)\nplt.bar(\n    br3,\n    metrics_1_shot[2],\n    color=\"mediumseagreen\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[2],\n)\n\n# Adding Xticks\nplt.xlabel(\"Metric\", fontweight=\"bold\", fontsize=15)\nplt.ylabel(\"Results\", fontweight=\"bold\", fontsize=15)\nplt.xticks(\n    [r + barWidth for r in range(len(metrics_1_shot[0]))], plt_metrics, fontsize=12\n)\nplt.ylim(0, 1)\nplt.title(\"Different metrics with 1-shot prompt for the models\", fontsize=20)\n\nplt.legend()\nplt.savefig(\"bars1.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen using the 1-shot prompt it is possible to see that the results are slightly better for the closed-source model.\nAdditionally, is possible to see that the fine-tuned model perform worse than the vanilla Llama3-8B model. As pointed above, this is because the fine-tuned model is seeing a format that is not the one that saw during training. Because of that, and having these results to prove it, we recommend avoiding the use of few-shot prompts with fine-tuned models.\n\n\nCode\n# set width of bar\nbarWidth = 0.15\nfig = plt.subplots(figsize=(20, 10))\n\nplt_models = [\"Llama3-8B 0-shot\", \"Llama3-8B 1-shot\", \"Llama3-8B Fine-tuned 0-shot\", \"Llama3-8B Fine-tuned 1-shot\", \"GPT-4o 0-shot\", \"GPT-4o 1-shot\"]\nplt_metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\nplt_data = {}\nfor index, model in enumerate(plt_models):\n    plt_data[model] = metrics_[index]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(metrics_[0]))\nbr2 = [x + barWidth for x in br1]\nbr3 = [x + barWidth for x in br2]\nbr4 = [x + barWidth for x in br3]\nbr5 = [x + barWidth for x in br4]\nbr6 = [x + barWidth for x in br5]\n\n# Make the plot\nplt.bar(\n    br1,\n    metrics_[0],\n    color=\"blue\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[0],\n)\nplt.bar(\n    br2,\n    metrics_[1],\n    color=\"darkblue\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[1],\n)\nplt.bar(\n    br3,\n    metrics_[2],\n    color=\"lightskyblue\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[2],\n)\nplt.bar(\n    br4,\n    metrics_[3],\n    color=\"deepskyblue\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[3],\n)\nplt.bar(\n    br5,\n    metrics_[4],\n    color=\"springgreen\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[4],\n)\nplt.bar(\n    br6,\n    metrics_[5],\n    color=\"seagreen\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[5],\n)\n\n# Adding Xticks\nplt.xlabel(\"Metric\", fontweight=\"bold\", fontsize=15)\nplt.ylabel(\"Results\", fontweight=\"bold\", fontsize=15)\nplt.xticks(\n    [r + 2.5 * barWidth for r in range(len(metrics_[0]))], plt_metrics, fontsize=12\n)\nplt.ylim(0, 1)\nplt.title(\"Different metrics using both prompts for the models\", fontsize=20)\nplt.legend(loc='lower right')\n\nplt.savefig(\"another.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nComparing all the results together allow us to see that the results with the 1-shot prompt are better for the three models compared to when using the 0-shot prompt.\nAlso, it is possible to observe that the closed-source model with the 1-shot prompt achieves the best result overall with an F1-score higher than 0.95.\nIn addition, it is important to point again the not so good results of the fine-tuned model when using the 1-shot prompt. Again this is because during the fine-tuning the model gets very used to a robust format, and that format is broken with the 1-shot. Commonly, when fine-tuning few-shot prompts are not used with the fine-tuned model due to this reason.\nFinally, claim that the differences are that small probably because of the method used to evaluate. Since the BERTScore method measures the similarity token by token, when applied to a JSON schema in which a lot of tokens are curly brackets and other tokens related with the schema, the results of these metrics are not very reliable. For more robust evaluations, consult the evaluations notebook[REF].",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#references",
    "href": "finetune/choosing_paradigm.html#references",
    "title": "4 | Choosing the learning paradigm",
    "section": "References",
    "text": "References\n\n\n\n\nAi, Qianxiang, Fanwang Meng, Jiale Shi, Brenden Pelkie, and Connor W. Coley. 2024. “Extracting Structured Data from Organic Synthesis Procedures Using a Fine-Tuned Large Language Model,” April. https://doi.org/10.26434/chemrxiv-2024-979fz.\n\n\nDettmers, Tim, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022. “8-Bit Optimizers via Block-Wise Quantization.” https://arxiv.org/abs/2110.02861.\n\n\nDettmers, Tim, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. “QLoRA: Efficient Finetuning of Quantized LLMs.” https://arxiv.org/abs/2305.14314.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2019. “Decoupled Weight Decay Regularization.” https://arxiv.org/abs/1711.05101.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "obtaining_data/annotation.html",
    "href": "obtaining_data/annotation.html",
    "title": "Data annotation",
    "section": "",
    "text": "{#sec-annotation}\nFor the manual data annotation one should use an automatically processable file format like JSON format, which provides the data in key-value pairs.\n\n{\n    \"key\": \"value\",\n    \"key2\": \"value 2\"\n}\n\nTherefore one could store the desired annotated data in an structured form. For example a JSON file of a Buchwald-Hartwig reaction could look like this:\n\n{\n    \"reaction_name\": \"Buchwald-Hartwig reaction\",\n    \"reactants\": [\n        \"5-Bromo-m-xylene\",\n        \"Benzylmethylamine\"\n    ],\n    \"catalyst\": [\n        \"Bis(dibenzylideneacetone)palladium(0)\",\n        \"Tri(o-tolyl)phosphine\"\n    ],\n    \"base\": \"Sodium tert-butoxide\",\n    \"solvent\": \"Toluene\",\n    \"temperature\": 65,\n    \"temperature_unit\": \"°C\",\n    \"product\": \"N-Benzyl-N-methyl(3,5-xylyl)amine\",\n    \"yield\": 88\n}\n\nTo make the process faster and error-free one could use an annotation tool like doccano.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data annotation</span>"
    ]
  },
  {
    "objectID": "document_parsing_and_cleaning/document_parsing_and_cleaning.html",
    "href": "document_parsing_and_cleaning/document_parsing_and_cleaning.html",
    "title": "2 | Document parsing and cleaning",
    "section": "",
    "text": "As a second step of the data extraction one should process the obtained data to: 1. obtain a format like markup language that can be used as input for text-based models (Parsing with NOUAGAT) 2. remove irrelevant information from the raw data to make the system more robust and cheaper (Data cleaning)",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>2 | Document parsing and cleaning</span>"
    ]
  }
]