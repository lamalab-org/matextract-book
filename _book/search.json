[
  {
    "objectID": "obtaining_data/obtaining_data.html",
    "href": "obtaining_data/obtaining_data.html",
    "title": "1 | Obtaining data for data extraction",
    "section": "",
    "text": "At the start of each data extraction process one has to first collect a dataset containing all data sources relevant for the extraction topic. Various Python packages are available to make this process easier and more efficient.\n\nCrossref can be used to search for relevant articles and metadata\nData can be mined from various sources such as ChemRxiv",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>1 | Obtaining data for data extraction</span>"
    ]
  },
  {
    "objectID": "obtaining_data/crossref_search.html",
    "href": "obtaining_data/crossref_search.html",
    "title": "1.1 | Obtaining a set of relevant data sources",
    "section": "",
    "text": "At the start of the data extraction process you have to collect a set of potentially relevant data sources. Therefore, you could collect a dataset manually or use a tool to help to automate and speed up this process.\nThe Crossref API is a very useful tool to collect the metadata of relevant articles. Besides the API there are multiple Python libraries available that make access to the API easier. One of these libraries is crossrefapi. As an example, 100 sources including metadata on the topic ‘buchwald-hartwig coupling’ are extracted and saved into a JSON file.\n\nfrom crossref.restful import Works\nimport json\n\nworks = Works(timeout=60)\n\n# Performing the search for sources on the topic of buchwald-hartwig coupling for 10 papers\nquery_result = works.query(bibliographic='buchwald-hartwig coupling').select('DOI', 'title', 'author', 'type', 'publisher', 'issued').sample(10)\n\nresults = [item for item in query_result]\n\n# Save 100 results including their metadata in a json file\nwith open('buchwald-hartwig_coupling_results.json', 'w') as file:\n    json.dump(results, file)\n\nprint(results)\n\nWith the obtained metadata one could afterwards try to filter for relevant or available data sources which could be downloaded through an API provided by the publishers or obtain from a data dump.\nAn example for using such an article downloading API is provided in the chapter about data mining.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 | Obtaining a set of relevant data sources</span>"
    ]
  },
  {
    "objectID": "obtaining_data/data_mining.html",
    "href": "obtaining_data/data_mining.html",
    "title": "1.2 | Mining data from ChemRxiv",
    "section": "",
    "text": "There are multiple datasets available which are open for data mining.\nTo download full text documents from open access databases the paperscraper tool can be used.\nAs an example, we will download full text articles from ChemRxiv on the topic of ‘buchwald-hartwig coupling’.\n\nfrom paperscraper.get_dumps import chemrxiv\n\n# Download of the ChemRxiv paper dump\nchemrxiv(save_path='chemrxiv_2020-11-10.jsonl')\n\n\nfrom paperscraper.xrxiv.xrxiv_query import XRXivQuery\nfrom paperscraper.pdf import save_pdf_from_dump\nimport pandas as pd\n\ndf = pd.read_json('./chemrxiv_2020-11-10.jsonl', lines=True)\n\n# define keywords for the paper search\nsynthesis = ['synthesis']\nreaction = ['buchwald-hartwig']\n\n# combine keywords using \"AND\" logic, i.e. search for papers that contain both keywords\nquery = [synthesis, reaction]\n\n# start searching for relevant papers in the ChemRxiv dump\nquerier = XRXivQuery('./chemrxiv_2020-11-10.jsonl')\nquerier.search_keywords(query, output_filepath='buchwald-hartwig_coupling_ChemRxiv.jsonl')\n\n# Save PDFs in current folder and name the files by their DOI\nsave_pdf_from_dump('./buchwald-hartwig_coupling_ChemRxiv.jsonl', pdf_path='./PDFs', key_to_save='doi')\n\nFor further steps in the data extraction process annotated data in needed to evaluate the extraction pipeline. For this, one could use an annotation tool like doccano which is shown in the following example.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>1.2 | Mining data from ChemRxiv</span>"
    ]
  },
  {
    "objectID": "constrained_decoding/index.html",
    "href": "constrained_decoding/index.html",
    "title": "Constrained generation to guarantee syntactic correctness",
    "section": "",
    "text": "Defining a data schema\nFor most constrained generation tasks, we need to define a data schema in a programmatic way. The most common way to do so is to use pydantic data classes. Here is an example of a simple data schema for a recipe:\nThis schema can also be extended to include descriptions of different fields or to only allow certain values for specific fields. For example, we could add a field for the number of servings and only allow positive integers.\nIf we want to extract copolymerization reactions a data schema could look like the following.\nWe can now use instructor to “patch” the OpenAI API client to ensure that our output fulfils the schema.\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\n\nclass Monomer(BaseModel):\n    name: str = Field(..., title=\"Name\", description=\"Name of the monomer.\")\n    reactivity_constant: Optional[float] = Field(\n        None,\n        title=\"Reactivity constant\",\n        description=\"Reactivity constant of the monomer. r1 for monomer 1 and r2 for monomer 2. Must be greater or equal 0.\",\n        ge=0,\n    )\n    reactivity_constant_error: Optional[float] = Field(\n        None,\n        title=\"Reactivity constant error\",\n        description=\"Error in the reactivity constant. Often indicated with +/-. Must be greater or equal 0\",\n        ge=0,\n    )\n    q_parameter: Optional[float] = Field(\n        None,\n        title=\"Q parameter\",\n        description=\"Q parameter of the monomer. Q1 for monomer 1 and Q2 for monomer 2. Must be greater or equal 0\",\n        ge=0,\n    )\n    e_parameter: Optional[float] = Field(\n        None,\n        title=\"e parameter\",\n        description=\"e parameter of the monomer. e1 for monomer 1 and e2 for monomer 2.\",\n    )\n\n\nclass CopolymerizationReaction(BaseModel):\n    temperature: Optional[float] = Field(\n        ...,\n        title=\"Temperature\",\n        description=\"Temperature at which the reaction is carried out\",\n    )\n    temperature_unit: Optional[Literal[\"C\", \"K\"]] = Field(\n        ..., title=\"Temperature unit\", description=\"Unit of temperature\"\n    )\n    solvent: Optional[str] = Field(\n        None,\n        title=\"Solvent\",\n        description=\"Solvent used in the reaction. If bulk polymerization was performed, this field should be left empty\",\n    )\n    initiator: Optional[str] = Field(\n        None, title=\"Initiator\", description=\"Initiator used in the reaction\"\n    )\n    monomers: Optional[List[Monomer]] = Field(\n        ...,\n        title=\"Monomers\",\n        description=\"Monomers used in the reaction. Ensure that the reactivity ratios are not confused with other numbers (such as Q and e). The two monomers MUST be used in the same reaction and mentioned in the same context.\",\n        min_items=2,\n        max_items=2,\n    )\n    polymerization_type: Optional[str] = Field(\n        ...,\n        title=\"Polymerization type\",\n        description=\"Type of polymerization (e.g., bulk, solution, suspension, emulsion)\",\n    )\n    determination_method: Optional[str] = Field(\n        ...,\n        title=\"Determination method\",\n        description=\"Method used to determine the reactivity ratios (e.g. Kelen Tudor, Fineman-Ross, Mayo-Lewis).\",\n    )\ndiagram = erd.create(CopolymerizationReaction)\ndiagram.draw(\"diagram.svg\")\nSVG(\"diagram.svg\")\nIn this case, we will use PDF files in the form as images as input for the model. To perform this conversion, we import some utilities.\nfrom pdf2image import convert_from_path\nfrom utils import process_image, get_prompt_vision_model\nThe code below only converts each page of the PDF into an image and then generates dictionary objects in a format that can be used by the OpenAI API.\nfilepath = 'paper01.pdf'\npdf_images = convert_from_path(filepath)\n\nimages_base64 = [process_image(image, 2048, 'images', filepath, j)[0] for j, image in enumerate(pdf_images)]\nimages = get_prompt_vision_model(images_base64=images_base64)\nArmed with the images, we can now use the OpenAI API to extract the text from the images. For this, we just call the API with our prompts and the images.\ncompletion = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    response_model=List[CopolymerizationReaction],\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are a scientific assistant, extracting accurate information about co-polymerization reactions from scientific papers.\nDo not use data that was reproduced from other sources.\nIf you confuse the reactivity ratios with other numbers, you will be penalized.\nMonomer names might be quite similar, if you confuse them, you will be penalized.\nNEVER combine data from different reactions, otherwise you will be penalized.\nIf you are unsure, return no data. Quality is more important than quantity.\n\"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"Extract the data from the paper into the provided data schema. We want an iterable of reaction objects and each reaction will be its own object. You can find each page of the paper as an image below.\nThe relationship between monomers and parameters is typically indicated by subscripts that can be a number or an abbreviation of the monomer.\nNever return data that you are not absolutely sure about! You will be penalized for incorrect data.\"\"\",\n        },\n        {\"role\": \"user\", \"content\": [*images]},\n    ],\n    temperature=0,\n)\ncompletion\n\n[CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='carbon tetrachloride', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.54, reactivity_constant_error=0.01, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.06, reactivity_constant_error=0.03, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='chloroform', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.51, reactivity_constant_error=0.01, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.08, reactivity_constant_error=0.03, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='acetone', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.43, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.65, reactivity_constant_error=0.02, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='1,4-dioxane', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.41, reactivity_constant_error=0.02, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.59, reactivity_constant_error=0.05, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='acetonitrile', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.12, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.29, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos')]",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Constrained generation to guarantee syntactic correctness</span>"
    ]
  },
  {
    "objectID": "constrained_decoding/index.html#defining-a-data-schema",
    "href": "constrained_decoding/index.html#defining-a-data-schema",
    "title": "Constrained generation to guarantee syntactic correctness",
    "section": "",
    "text": "from pydantic import BaseModel\n\nclass Recipe(BaseModel):\n    title: str\n    ingredients: List[str]\n    instructions: List[str]\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, List\n\nclass Recipe(BaseModel):\n    title: str\n    ingredients: List[str]\n    instructions: List[str]\n    servings: int = Field(..., gt=0, description=\"The number of servings for this recipe\")\n    rating: Literal[\"easy\", \"medium\", \"hard\"] = Field(\"easy\", description=\"The difficulty level of this recipe\")",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Constrained generation to guarantee syntactic correctness</span>"
    ]
  },
  {
    "objectID": "ocr/nougat/nougat.html",
    "href": "ocr/nougat/nougat.html",
    "title": "OCR with Nougat",
    "section": "",
    "text": "Cleaning the data\nFor certain applications it might be necessary to clean the data before using it for other downstream tasks.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>OCR with Nougat</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative structured data extraction using LLMs",
    "section": "",
    "text": "About this book\nStructured data is at the heart of machine learning. LLMs offer a convenient way to generate structured data based on unstructured inputs. This book gives hands-on examples of the different steps in the extraction workflow using LLMs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Generative structured data extraction using LLMs</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html",
    "href": "finetune/choosing_paradigm.html",
    "title": "4 | Choosing the learning paradigm",
    "section": "",
    "text": "First steps\nChoosing the learning paradigm should begin trying some models with a general pre-training. For this practical case, the first model to test is the very recent Llama-3 8B model with a zero and one-shot prompts.\nWe will start by importing all the packages needed.\nimport json \nimport wandb\nfrom dotenv import load_dotenv\n\nimport torch\nfrom datasets import (\n    load_dataset,\n)\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n)\nfrom peft import (\n    LoraConfig,\n)\nfrom trl import (\n    SFTTrainer,\n    DataCollatorForCompletionOnlyLM,\n)\nfrom evaluate import load\nimport litellm\nfrom litellm import completion\nfrom litellm.caching import Cache\nfrom statistics import mean\nimport numpy as np \nimport matplotlib.pyplot as plt\nTo continue we will allow for the caching of LiteLLM. Additionally, we will import all environment variables.\nlitellm.cache = Cache()\nload_dotenv(\".env\", override=True)",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#first-steps",
    "href": "finetune/choosing_paradigm.html#first-steps",
    "title": "4 | Choosing the learning paradigm",
    "section": "",
    "text": "Note that using the environment variables is the safest way of keeping personal API keys secret.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#first-model-and-dataset",
    "href": "finetune/choosing_paradigm.html#first-model-and-dataset",
    "title": "4 | Choosing the learning paradigm",
    "section": "First model and dataset",
    "text": "First model and dataset\n\n\nIt is important to include the HF_token in the .env file. By the time this notebook was created, the model that we are going to fine-tune (Llama3-8B) is only available after request for access.\nAs starting model, we will try the Llama-3 8B model. We will call this model through the Groq API, which allows performing fast inference with several open-source models.\n\n\nGroq is a provider of some of the most popular open-source models such as Llama or Mixtral models with a high inference speed. To use the Groq API, it also needed to add the GROQ_API_KEY to the .env file.\n\nbase_model = \"groq/llama3-8b-8192\"\n\nThe dataset used in this tutorial is the one used in Ai et al.’s (Ai et al. 2024) recent work, which contains data about chemical reactions. The dataset contains 100K reaction procedure—ORD JSON pairs.\n\n\nORD stands for Open Reaction Database schema, a comprehensive data structure specially designed to describe all the elements involved in chemical reactions.\n\n\n\n\n\n\nDownload data\n\n\n\n\n\nTo obtain the data, the best way is to install the GitHub repository of the Ai et al. work and take from there. To do so, run the following commands:\n!git clone https://github.com/qai222/LLM_organic_synthesis.git\n!cp LLM_organic_synthesis/workplace_data/datasets/USPTO-n100k-t2048_exp1.7z .\n!7za x USPTO-n100k-t2048_exp1.7z\n!cp USPTO-n100k-t2048_exp1/*.json .\n!rm -rf USPTO-n100k-t2048_exp1/ USPTO-n100k-t2048_exp1.7z LLM_organic_synthesis\nThis will leave four .json files in your current directory that contain all the data used here.\n\n\n\n\ntest_ds_path = \"test.json\"\ntest_dataset = load_dataset(\"json\", data_files=test_ds_path, split=\"train\")\ntest_dataset = test_dataset.shuffle(seed=42).select(range(100))\ntest_dataset\n\nDataset({\n    features: ['output', 'instruction'],\n    num_rows: 100\n})\n\n\n\ntest_dataset[0]\n\n{'output': '{\"inputs\": {\"m1\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"4-nitrobenzyl (1R,3R,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-oxo-1-carbapenam-3-carboxylate\"}], \"amount\": {\"mass\": {\"value\": 743.0, \"units\": \"MILLIGRAM\"}}, \"reaction_role\": \"REACTANT\"}]}, \"m2\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"2-(tri-n-butylstannyl)-7-trifluoromethylthioimidazo[5,1-b]thiazole\"}], \"amount\": {\"mass\": {\"value\": 1.06, \"units\": \"GRAM\"}}, \"reaction_role\": \"REACTANT\"}]}}, \"conditions\": {\"conditions_are_dynamic\": true}, \"outcomes\": [{\"products\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"4-nitrobenzyl (1S,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-(7-trifluoromethylthioimidazo[5,1-b]thiazol-2-yl)-1-carbapen-2-em-3-carboxylate\"}], \"measurements\": [{\"type\": \"AMOUNT\", \"details\": \"MASS\", \"amount\": {\"mass\": {\"value\": 172.0, \"units\": \"MILLIGRAM\"}}}], \"reaction_role\": \"PRODUCT\"}]}]}',\n 'instruction': 'Below is a description of an organic reaction. Extract information from it to an ORD JSON record.\\n\\n### Procedure:\\nThe procedure of Example 1a) was repeated, except that 743 mg of 4-nitrobenzyl (1R,3R,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-oxo-1-carbapenam-3-carboxylate and 1.06 g 2-(tri-n-butylstannyl)-7-trifluoromethylthioimidazo[5,1-b]thiazole were used as the starting compounds. Thus, 172 mg of 4-nitrobenzyl (1S,5R,6S)-6-((1R)-1-hydroxyethyl)-1-methyl-2-(7-trifluoromethylthioimidazo[5,1-b]thiazol-2-yl)-1-carbapen-2-em-3-carboxylate was prepared.\\n\\n### ORD JSON:\\n'}\n\n\n\n\nNote that the output comes in JSON format. For this simple example we are not going to constrain the output to JSON format. However, to ensure good results this constraining must be done. Deep explanation and examples about the options and how to constrain the output are provided in the chapter and its notebook “Constrained decoding and enforcing valid outputs”.\nThis dataset is very big. Therefore, we will only take 100 samples from the test set used in the article mentioned above for our test set.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#prompt-and-inference",
    "href": "finetune/choosing_paradigm.html#prompt-and-inference",
    "title": "4 | Choosing the learning paradigm",
    "section": "Prompt and Inference",
    "text": "Prompt and Inference\nWe define a simple prompt template. The prompt contains a simple system part (named as PREFIX) where the role and task of the model is defined as well as the example used only for the 1-shot prompt. Additionally, the prompt has a user prompt where the reaction instruction will be provided.\n\nPREFIX = \"\"\"You are a helpful scientific assistant. Your task is to extract information about organic reactions. {shot}\"\"\"\nSUFFIX = \"\"\"\\n\\n{sample}\\n\\n\"\"\"\nSHOT = \"\"\"\nOne example is provided to you to show how to perform the task:\n\n### Procedure:\\nA suspension of 8 g of the product of Example 7 and 0.4 g of DABCO in 90 ml of xylenes were heated under N2 at 130\\u00b0-135\\u00b0 C. while 1.8 ml of phosgene was added portionwise at a rate to maintain a reflux temperature of about 130\\u00b0-135\\u00b0 C. The mixture was refluxed an additional two hours, cooled under N2 to room temperature, filtered, and the filtrate was concentrated in vacuo to yield 6.9 g of the subject compound as a crude oil.\\n\\n\n### ORD JSON:\\n{\\\"inputs\\\": {\\\"m1_m2_m4\\\": {\\\"components\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"product\\\"}], \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 8.0, \\\"units\\\": \\\"GRAM\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}, {\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"DABCO\\\"}], \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 0.4, \\\"units\\\": \\\"GRAM\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}, {\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"xylenes\\\"}], \\\"amount\\\": {\\\"volume\\\": {\\\"value\\\": 90.0, \\\"units\\\": \\\"MILLILITER\\\"}}, \\\"reaction_role\\\": \\\"SOLVENT\\\"}]}, \\\"m3\\\": {\\\"components\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"phosgene\\\"}], \\\"amount\\\": {\\\"volume\\\": {\\\"value\\\": 1.8, \\\"units\\\": \\\"MILLILITER\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}]}}, \\\"conditions\\\": {\\\"temperature\\\": {\\\"control\\\": {\\\"type\\\": \\\"AMBIENT\\\"}}, \\\"conditions_are_dynamic\\\": true}, \\\"workups\\\": [{\\\"type\\\": \\\"ADDITION\\\", \\\"details\\\": \\\"was added portionwise at a rate\\\"}, {\\\"type\\\": \\\"TEMPERATURE\\\", \\\"details\\\": \\\"to maintain a reflux temperature of about 130\\\\u00b0-135\\\\u00b0 C\\\"}, {\\\"type\\\": \\\"TEMPERATURE\\\", \\\"details\\\": \\\"The mixture was refluxed an additional two hours\\\", \\\"duration\\\": {\\\"value\\\": 2.0, \\\"units\\\": \\\"HOUR\\\"}}, {\\\"type\\\": \\\"FILTRATION\\\", \\\"details\\\": \\\"filtered\\\"}, {\\\"type\\\": \\\"CONCENTRATION\\\", \\\"details\\\": \\\"the filtrate was concentrated in vacuo\\\"}], \\\"outcomes\\\": [{\\\"products\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"subject compound\\\"}], \\\"measurements\\\": [{\\\"type\\\": \\\"AMOUNT\\\", \\\"details\\\": \\\"MASS\\\", \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 6.9, \\\"units\\\": \\\"GRAM\\\"}}}], \\\"reaction_role\\\": \\\"PRODUCT\\\"}]}]}\n\\n\n\"\"\"\n\n\n\nPREFIX is supposed to be content of the system prompt. SUFFIX is the user prompt. SHOT is the 1-shot prompt that will be added to the system prompt when used.\nTo continue, we loop all over the dataset two times, one for each type of prompt (zero and one-shot). For each dataset sample, we format the prompt to include the procedure-output schema pairs using the template defined in the previous cell. In addition, we also predict using the model and store those predictions for future evaluation.\n\nshots = ['0-shot', '1-shot']\nresults_llama = {}\n\n# Start by looping over the shots\nfor s in shots:\n    predictions = []\n    references = []\n\n# Loop over all the samples of the dataset\n    for t in test_dataset:\n        instruction = t[\"instruction\"]\n        output = t[\"output\"]\n        # Format the prompt following OpenAI's prompting guidelines\n        if s == '0-shot':\n            shot = \"\"\n        else:\n            shot = SHOT\n        system = PREFIX.format(shot=shot)\n        user = SUFFIX.format(sample=instruction)\n        prompt = [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user},\n        ]\n        # Do the completion using Groq API through LiteLLM\n        pred = (\n            completion(\n                model=base_model,\n                messages=prompt,\n                caching=True,\n                temperature=0,\n            )\n            .choices[0]\n            .message.content\n        )\n        # Save the predictions and the references for later evaluation\n        references.append(output)\n        predictions.append(pred)\n\n    results_llama[s] = {\n        \"predictions\": predictions,\n        \"references\": references,\n    }\n\n\n\nThe beauty of LiteLLMis that it allows to do completions for a bunch of models by different providers using for all the OpenAI prompt completions schema: {“role”: “system”, “content”: system_message}, {“role”: “user”, “content”: user_message}\nThis format of completions is probably the most extended one, and is really intuitive.\nAfter generating the predictions, it’s essential to evaluate them. We will initially use BERT Score for a simple evaluation, as it provides precision, recall, and F\\(_1\\)-scores based on similarity measures. However, for a complex schema like the one we are predicting, a more robust evaluation method should be utilized. (REF EVALUATION CHAPTER)\n\nbertscore = load(\"bertscore\")\nshots = ['0-shot', '1-shot']\n\n# Start by looping over the shots\nfor s in shots:\n    predictions = results_llama[s][\"predictions\"]\n    references = results_llama[s][\"references\"]\n\n    results_ = bertscore.compute(\n        predictions=predictions,\n        references=references,\n        model_type=\"distilbert-base-uncased\",\n    )\n\n    results_llama[s].update(\n        {\n            \"precision\": mean(results_[\"precision\"]),\n            \"recall\": mean(results_[\"recall\"]),\n            \"f1_scores\": mean(results_[\"f1\"]),\n        }\n    )\n\n\nfor s in shots:\n    print(f\"Results for the {s} prompt\")\n    print(f\"\\tPrecision: {results_llama[s]['precision']}\")\n    print(f\"\\tRecall: {results_llama[s]['recall']}\")\n    print(f\"\\tF1-Score: {results_llama[s]['f1_scores']}\")\n\nThe results are really good, specially with the 1-shot prompt. However we are going to try next another different model, a closed-source in order to compare.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#another-model-closed-source-this-time",
    "href": "finetune/choosing_paradigm.html#another-model-closed-source-this-time",
    "title": "4 | Choosing the learning paradigm",
    "section": "Another model, closed-source this time",
    "text": "Another model, closed-source this time\nThe second model we will use is the newer OpenAI model, the GPT-4o model. This allows us to compare open- and closed-source models.\nThe procedure and code are exactly the same as for the previous case; the only difference is to define a different model.\n\nbase_model = \"gpt-4o\"\n\nAnd we do the completions using both prompts for all the test samples.\n\n\nOpenAI models are also supported by the LiteLLM package.\n\nresults_openai = {}\nshots = [\"0-shot\", \"1-shot\"]\n\n# Start by looping over the shots\nfor s in shots:\n    predictions = []\n    references = []\n\n# Loop over all the samples of the dataset\n    for t in test_dataset:\n        instruction = t[\"instruction\"]\n        output = t[\"output\"]\n        # Format the prompt following OpenAI's prompting guidelines\n        if s == \"0-shot\":\n            shot = \"\"\n        else:\n            shot = SHOT\n        system = PREFIX.format(shot=shot)\n        user = SUFFIX.format(sample=instruction)\n        prompt = [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user},\n        ]\n        # Do the completion using Groq API through LiteLLM\n        pred = (\n            completion(\n                model=base_model,\n                messages=prompt,\n                caching=True,\n                temperature=0,\n            )\n            .choices[0]\n            .message.content\n        )\n        # Remove some residual stuff in the json output by the model.\n        if \"```json\" in pred:\n            pred = pred.replace(\"```json\\n\", \"\")\n            pred = pred.replace(\"```\", \"\")\n            \n        # Save the predictions and the references for later evaluation\n        references.append(output)\n        predictions.append(pred)\n\n    results_openai[s] = {\n        \"predictions\": predictions,\n        \"references\": references,\n    }\n\nFinally, we evaluate again using BERT-score.\n\nfor s in shots:\n    predictions = results_openai[s]['predictions']\n    references = results_openai[s]['references']\n\n    results_ = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n\n    results_openai[s].update({\n        'precision': mean(results_['precision']),\n        'recall': mean(results_['recall']),\n        'f1_scores': mean(results_['f1']),\n    })\n\n\nfor s in shots:\n    print(f\"Results for the {s} prompt\")\n    print(f\"\\tPrecision: {results_llama[s]['precision']}\")\n    print(f\"\\tRecall: {results_llama[s]['recall']}\")\n    print(f\"\\tF1-Score: {results_llama[s]['f1_scores']}\")\n\nThe results with this GPT-4o model are really good, improving a bit the ones obtained with Llama-3 8B base model. However, we are going to try to further improve this results by fine-tuning the Llama-3 8B model.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#fine-tuning",
    "href": "finetune/choosing_paradigm.html#fine-tuning",
    "title": "4 | Choosing the learning paradigm",
    "section": "Fine-tuning",
    "text": "Fine-tuning\nAs the final step, we will fine-tune the Llama-3 8B using data similar to the one we used above.\nWe will use packages built by HuggingFace to do the fine-tuning.\nFirst, we define the base model we will use and the path of the dataset.\n\n# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n\n\nThis will make it easy to track training progress using Weights and Biases, allowing quick access to the loss curve and other important training parameters.\n\nwandb.login()\n\n\n\nIn order the wandb.login()works, the .env imported previously must contain the personal WANDB_API_KEY.\nThe next step is to load the dataset for the fine-tuning. For that, as for the testing from previous models, we will use the dataset used by Ai et al., but for this case, we will use their train dataset. Since this is a quick demonstration, we will only take 5000 samples.\n\ndataset = load_dataset(\"json\", data_files='train.json', split=\"train\")\ndataset = dataset.shuffle(seed=42).select(range(5000)) # Only use 5000 samples for quick demo\ndataset = dataset.train_test_split(test_size=0.1, seed=42) # We define 90-10 % training-evaluation splits.\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['output', 'instruction'],\n        num_rows: 4500\n    })\n    test: Dataset({\n        features: ['output', 'instruction'],\n        num_rows: 500\n    })\n})\n\n\nThen we define the method to fine-tune and all the parameters. For this fine-tuning we will be using the popular QLoRA method. QLoRA (Dettmers et al. 2023) is an efficient finetuning approach that reduces memory usage during finetuning, while preserving full finetuning task performance.\n\n\n\n\n\n\nNotes about QLoRA configuration\n\n\n\n\n\n\nload_in_4bit=True: loads the model using the 4-bit quatization.\nbnb_4bit_quant_type=“nf4”: quatizacies following the nf4 method.(Dettmers et al. 2022)\nbnb_4bit_use_double_quant=True: activate nested quantization for 4-bit base models.\nbnb_4bit_compute_dtype=torch.bfloat16: Compute dtype for 4-bit base models.\n\n\n\n\n\n# QLoRA configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\", # fp4 or nf4\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n\n\n\n\n\n\nNotes about LoRA configuration\n\n\n\n\n\n\nr: The rank of the updated matrices, expressed as integer. Lower rank results in smaller update matrices with fewer trainable parameters. It means that the adaptor that is build in top of the model to improved will be made by matrices of rank 32.\nlora_alpha: LoRA scaling factor. It changes how the adaptation layer’s weights affect the base model’s.\nlora_dropout: Dropout is a regularization technique where a proportion of neurons (or parameters) are randomly “dropped out” or turned off during training to prevent overfitting.\nbias: Specifies if the bias parameters should be trained. Can be ‘none’, ‘all’ or ‘lora_only’.\ntask_type: Task to perform, “Causal LM”: Causal language modeling.\n\n\n\n\n\npeft_config = LoraConfig(\n    r=32,\n    lora_alpha=64, \n    lora_dropout=0.1,\n    bias=\"none\", \n    task_type=\"CAUSAL_LM\", \n)\n\nBefore training, we define the tokenizer and the model for fine-tuning, set the training arguments, and initialize the trainer.\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model) # Define the tokenizer\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Where the \"pad_token\" is placed\n\n# Model config\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model, # Model that we are going to fine-tune\n    quantization_config=bnb_config, # QLoRA config defined above\n    device_map=\"auto\", # Where the model is trained, set device_map=\"auto\" loads a model onto available GPUs first.\n)\n\n\n\nThe pad_token is a special token used to make arrays of tokens the same size for batching purpose. The typical is to use the “eos_token” which is a special token representing the end of a sentence.\n\n\n\n\n\n\nNotes about the training arguments\n\n\n\n\n\n\nlearning_rate: the learning rate is a hyper-parameter that sets how the training algorithm updates the values of the weights.\nBatch size: it is the number of samples used in one forward and backward pass through the network. Ideally we would like to increase this number so the fine-tuning would last less. The problem is that for higher batch number, more GPU memory is needed. For example, for training the model used in this demostration using the exact same configuration but with a default token length (1024 tokens), with 40GB vRAM GPU, the maximum batch number is 2. Using 80GB of vRAM GPU the batch size can be increased to 4.\n\nper_device_train_batch_size: batch size for the training.\nper_device_eval_batch_size: batch size for the evaluation.\n\ngradient_accumulation_steps: Number of accumulated gradients over each batch.\noptim: optimizer used. The main role of the optimizer is to minimize the loss function. The paged_adamw_32bit is the well known AdamW optimizer. AdamW optimization is a stochastic gradient descent method.(Loshchilov and Hutter 2019)\nnum_train_epochs: number of times that the model goes through each sample during the training. Larger number might lead to best training results or to overfitting. Lower number might give a model that do not work as expected at all.\nfp16 and bf16: these parameters helps achieving mixed precision training which is a technique that aims to optimize the computational efficiency of training models by utilizing lower-precision numerical formats for certain variables. However, choosing of both parameters depends of the architecture of the GPUs that are being used during the training.\nlogging_steps: when the logging is done.\nevaluation_strategy: the evaluation strategy to adopt during training. The most used is ‘steps’ meaning that the evaluation is done after a certain number of training steps.\neval_steps: define in which steps the evaluation is done.\nmax_grad_norm: maximum gradient norm (for gradient clipping). Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and using the clipped gradients to update the weights.\nwarmup_steps: number of steps used for a linear warmup from 0 to learning_rate. The warmup helps to stabilize the optimization process and prevent divergence.\nwarmup_ratio: ratio of total training steps used for the linear warmup.\ngroup_by_length: whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding.\nlr_scheduler_type: describes the decayment of the learning rate during the training.\noutput_dir: directory to safe the report of the training process.\nsave_strategy: what we want to safe during the training. Set it to “no” to only safe the final model.\n\n Shape of the “cosine” lr_scheduler_type option\n\n\n\n\n# Define the different hyperparameters and arguments for the fine-tuning\ntraining_arguments = TrainingArguments(\n    learning_rate=1e-6,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=10,\n    fp16=False,\n    bf16=True, #bf16 to True with an A100, False otherwise\n    logging_steps=1, # Logging is done every step.\n    evaluation_strategy=\"steps\", \n    eval_steps=0.05,\n    max_grad_norm=0.3,\n    warmup_steps=10, \n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\", \n    output_dir=\"./results/\", \n    save_strategy='no', \n)\n\n\nresponse_template = \" ### Answer:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\n\nData collators are objects that will form a batch by using a list of dataset elements as input. There is one data collator for each task, here we use the one for completion-only.\n\nThe completion-only training instead of training the model on the whole input (prompt + answer) make the training more efficient by training only the model on completion.\n\n\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n        output_texts.append(text)\n    return output_texts\n\n\n\nThe formatting function is intended for those cases where the prompt is divided in more than one feature of the dataset. Using the a formatting functions allow us to join them in the best way.\n\ntrainer = SFTTrainer(\n    model=model, # Model to fine-tune\n    max_seq_length=2048, # Max number of tokens of the completion\n    args=training_arguments, # Training arguments to use \n    train_dataset=dataset[\"train\"], # Set of the dataset used for the training\n    eval_dataset=dataset[\"test\"], # Set of the dataset used for the evaluations\n    peft_config=peft_config, # Configuration and PEFT method to use\n    tokenizer=tokenizer, # Tokenizer used \n    packing=False,\n    formatting_func=formatting_prompts_func, # Prompt formatting function\n    data_collator=collator, \n)\n\n\n\nWhen packing is set to True means during the training, multiple short examples are pack in the same input sequence to increase training efficiency. However, when using a collator it must be False.\nAnd finally when everything is ready we train the model.\n\ntrainer.train()\n\n\n\n    \n      \n      \n      [ 5367/11250 2:46:41 &lt; 3:02:47, 0.54 it/s, Epoch 4.77/10]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n563\n0.467600\n0.592538\n\n\n1126\n0.404200\n0.572942\n\n\n1689\n0.509100\n0.566474\n\n\n2252\n0.408500\n0.563503\n\n\n2815\n0.553200\n0.562221\n\n\n3378\n0.472700\n0.561783\n\n\n3941\n0.495300\n0.561582\n\n\n4504\n0.469400\n0.561479\n\n\n5067\n0.545600\n0.561480\n\n\n\n\n\n\n\nWe can consult the loss curves from WandB to see if the training performs well and if there is no overfitting or other strange behavior.\n\n\n\n\n\n\n\n\n\n\n\n(a) Training-loss curve\n\n\n\n\n\n\n\n\n\n\n\n(b) Evaluation-loss curve\n\n\n\n\n\n\n\nFigure 6.1: Loss curves from the fine-tuning that were reported to WandB. Both curves looks such as they should be when the training goes fine.\n\n\n\nTo evaluate the fine-tuned model and do inference, the easiest way is to use the trained model directly. To do that we have to define a pipeline for text-generation and then do the inference using that pipeline and evaluate in a similar way as for the previous models.\n\n# Define the pipeline that will do the inference\nsft_pipe = pipeline(\n    \"text-generation\",\n    temperature=0.01,\n    model=trainer.model, # We do the inference with the trained model.\n    tokenizer=tokenizer,\n)\n\n\n\nThe temperature defines the freedom degrees that are allowed to the model. For data extraction the best value is 0 but HuggingFace do not allow a round 0, so we have to set a number very close to 0.\n\n# Perform the inference for all the samples of the test set using 0 and 1-shot prompts.\nresults_sft = {}\nshots = ['0-shot', '1-shot']\n\n# Start by looping over the shots\ncount = 0\nfor s in shots:\n    count+=1\n    print(count)\n    references = []\n    predictions_sft = []\n    prompts = []\n    # Loop over all the samples of the dataset\n    for t in test_dataset:\n        instruction = t['instruction']\n        output = t['output']\n        if s == '0-shot':\n            shot = ''\n        else:\n            shot = SHOT\n        # Format the prompt\n        system = PREFIX.format(shot=shot)\n        user = SUFFIX.format(sample=instruction)\n        prompt = system + user\n        references.append(output)\n        \n        # Do the completion using the pipeline defined above\n        with torch.cuda.amp.autocast():\n            pred = sft_pipe(prompt)\n        # Clean the output\n        predictions_sft.append(pred[0]['generated_text'].replace(prompt, ''))\n    \n    results_sft [s] = {\n        \"predictions\": predictions_sft,\n        \"references\": references,\n    }\n\n\n\nNote that for this particular case we do not use LiteLLM. This is because this package do not yet support Completion only task with HuggingFace models. Thus the prompt is not written following OpenAI completions guide.\n\nFor this case, note that in the completion we replace the prompt by nothing. This is done because the model was instructed to always give the completions with the prompt at the beginning. Above, for the case where we used this same Llama model, this was not done because the API already provides the completion without the prompt.\n\nFinally, we just need to calculate the metrics to evaluate this last model results.\n\nfor s in shots:\n    predictions_sft = results_sft[s][\"predictions\"]\n    references = results_sft[s][\"references\"]\n\n    results = bertscore.compute(predictions=predictions_sft, references=references, model_type=\"distilbert-base-uncased\")\n\n    results_sft[s].update({\n        \"precision\": mean(results[\"precision\"]),\n        \"recall\": mean(results[\"recall\"]),\n        \"f1_scores\": mean(results[\"f1\"]),\n    })\n\n\nfor s in shots:\n    print(f\"Results for the {s} prompt\")\n    print(f\"\\tPrecision: {results_sft[s]['precision']}\")\n    print(f\"\\tRecall: {results_sft[s]['recall']}\")\n    print(f\"\\tF1-Score: {results_sft[s]['f1_scores']}\")\n\nThe results are very similar as for the previous cases. But being honest with ourselfs, there was not a lot of room for big improvement.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#visualization-of-the-results",
    "href": "finetune/choosing_paradigm.html#visualization-of-the-results",
    "title": "4 | Choosing the learning paradigm",
    "section": "Visualization of the results",
    "text": "Visualization of the results\nTo study the results in a more graphical way, we can plot all the results in two bars plots.\n\n# set width of bar\nbarWidth = 0.2\nfig = plt.subplots(figsize=(20, 10))\n\nplt_models = [\"Llama3-8B\", \"Llama3-8B Fine-tuned\", \"GPT-4o\"]\nplt_metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\nplt_data = {}\nfor index, model in enumerate(plt_models):\n    plt_data[model] = metrics_0_shot[index]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(metrics_0_shot[0]))\nbr2 = [x + barWidth for x in br1]\nbr3 = [x + barWidth for x in br2]\n\n# Make the plot\nplt.bar(\n    br1,\n    metrics_0_shot[0],\n    color=\"b\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[0],\n)\nplt.bar(\n    br2,\n    metrics_0_shot[1],\n    color=\"skyblue\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[1],\n)\nplt.bar(\n    br3,\n    metrics_0_shot[2],\n    color=\"mediumseagreen\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[2],\n)\n\n# Adding Xticks\nplt.xlabel(\"Metric\", fontweight=\"bold\", fontsize=15)\nplt.ylabel(\"Results\", fontweight=\"bold\", fontsize=15)\nplt.xticks(\n    [r + barWidth for r in range(len(metrics_0_shot[0]))], plt_metrics, fontsize=12\n)\nplt.ylim(0, 1)\nplt.title(\"Different metrics with 0 shot prompt for the models\", fontsize=20)\n\nplt.legend()\nplt.savefig(\"bars0.png\")\nplt.show()\n\n\n# set width of bar\nbarWidth = 0.2\nfig = plt.subplots(figsize=(20, 10))\n\nplt_models = [\"Llama3-8B\", \"Llama3-8B Fine-tuned\", \"GPT-4-turbo\"]\nplt_metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\nplt_data = {}\nfor index, model in enumerate(plt_models):\n    plt_data[model] = metrics_1_shot[index]\n\n# Set position of bar on X axis\nbr1 = np.arange(len(metrics_1_shot[0]))\nbr2 = [x + barWidth for x in br1]\nbr3 = [x + barWidth for x in br2]\n\n# Make the plot\nplt.bar(\n    br1,\n    metrics_1_shot[0],\n    color=\"b\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[0],\n)\nplt.bar(\n    br2,\n    metrics_1_shot[1],\n    color=\"skyblue\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[1],\n)\nplt.bar(\n    br3,\n    metrics_1_shot[2],\n    color=\"mediumseagreen\",\n    width=barWidth,\n    edgecolor=\"black\",\n    label=plt_models[2],\n)\n\n# Adding Xticks\nplt.xlabel(\"Metric\", fontweight=\"bold\", fontsize=15)\nplt.ylabel(\"Results\", fontweight=\"bold\", fontsize=15)\nplt.xticks(\n    [r + barWidth for r in range(len(metrics_1_shot[0]))], plt_metrics, fontsize=12\n)\nplt.ylim(0, 1)\nplt.title(\"Different metrics with 1-shot prompt for the models\", fontsize=20)\n\nplt.legend()\nplt.savefig(\"bars1.png\")\nplt.show()\n\nThe results for the three models are quite good, and there are no significant differences between them. This is partly due to the evaluation method used; however, employing a more robust evaluation will lead to more noticeable differences. For this reason, we recommend reviewing the Evaluations section and following the guidelines outlined there.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  },
  {
    "objectID": "finetune/choosing_paradigm.html#references",
    "href": "finetune/choosing_paradigm.html#references",
    "title": "4 | Choosing the learning paradigm",
    "section": "References",
    "text": "References\n\n\nAi, Qianxiang, Fanwang Meng, Jiale Shi, Brenden Pelkie, and Connor W.\nColey. 2024. “Extracting Structured Data from Organic Synthesis\nProcedures Using a Fine-Tuned Large Language Model,” April. https://doi.org/10.26434/chemrxiv-2024-979fz.\n\n\nDettmers, Tim, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022.\n“8-Bit Optimizers via Block-Wise Quantization.” https://arxiv.org/abs/2110.02861.\n\n\nDettmers, Tim, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.\n2023. “QLoRA: Efficient Finetuning of Quantized LLMs.” https://arxiv.org/abs/2305.14314.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2019. “Decoupled Weight Decay\nRegularization.” https://arxiv.org/abs/1711.05101.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 | Choosing the learning paradigm</span>"
    ]
  }
]