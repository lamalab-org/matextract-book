[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative structured data extraction using LLMs",
    "section": "",
    "text": "About this book\nStructured data is at the heart of machine learning. LLMs offer a convenient way to generate structured data based on unstructured inputs. This book gives hands-on examples of the different steps in the extraction workflow using LLMs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Generative structured data extraction using LLMs</span>"
    ]
  },
  {
    "objectID": "obtaining_data/obtaining_data.html",
    "href": "obtaining_data/obtaining_data.html",
    "title": "1 | Obtaining data for data extraction",
    "section": "",
    "text": "At the start of each data extraction process one has to first collect a dataset containing all data sources relevant for the extraction topic. Various Python packages are available to make this process easier and more efficient.\n\nCrossref can be used to search for relevant articles and metadata\nData can be mined from various sources such as ChemRxiv",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>1 | Obtaining data for data extraction</span>"
    ]
  },
  {
    "objectID": "obtaining_data/crossref_search.html",
    "href": "obtaining_data/crossref_search.html",
    "title": "1.1 | Obtaining a set of relevant data sources",
    "section": "",
    "text": "At the start of the data extraction process you have to collect a set of potentially relevant data sources. Therefore, you could collect a dataset manually or use a tool to help to automate and speed up this process.\nThe Crossref API is a very useful tool to collect the metadata of relevant articles. Besides the API there are multiple Python libraries available that make access to the API easier. One of these libraries is crossrefapi. As an example, 100 sources including metadata on the topic ‘buchwald-hartwig coupling’ are extracted and saved into a JSON file.\n\nfrom crossref.restful import Works\nimport json\n\nworks = Works(timeout=60)\n\n# Performing the search for sources on the topic of buchwald-hartwig coupling for 10 papers\nquery_result = works.query(bibliographic='buchwald-hartwig coupling').select('DOI', 'title', 'author', 'type', 'publisher', 'issued').sample(10)\n\nresults = [item for item in query_result]\n\n# Save 100 results including their metadata in a json file\nwith open('buchwald-hartwig_coupling_results.json', 'w') as file:\n    json.dump(results, file)\n\nprint(results)\n\nWith the obtained metadata one could afterwards try to filter for relevant or available data sources which could be downloaded through an API provided by the publishers or obtain from a data dump.\nAn example for using such an article downloading API is provided in the chapter about data mining.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 | Obtaining a set of relevant data sources</span>"
    ]
  },
  {
    "objectID": "obtaining_data/data_mining.html",
    "href": "obtaining_data/data_mining.html",
    "title": "1.2 | Mining data from ChemRxiv",
    "section": "",
    "text": "There are multiple datasets available which are open for data mining.\nTo download full text documents from open access databases the paperscraper tool can be used.\nAs an example, we will download full text articles from ChemRxiv on the topic of ‘buchwald-hartwig coupling’.\n\nfrom paperscraper.get_dumps import chemrxiv\n\n# Download of the ChemRxiv paper dump\nchemrxiv(save_path='chemrxiv_2020-11-10.jsonl')\n\n\nfrom paperscraper.xrxiv.xrxiv_query import XRXivQuery\nfrom paperscraper.pdf import save_pdf_from_dump\nimport pandas as pd\n\ndf = pd.read_json('./chemrxiv_2020-11-10.jsonl', lines=True)\n\n# define keywords for the paper search\nsynthesis = ['synthesis']\nreaction = ['buchwald-hartwig']\n\n# combine keywords using \"AND\" logic, i.e. search for papers that contain both keywords\nquery = [synthesis, reaction]\n\n# start searching for relevant papers in the ChemRxiv dump\nquerier = XRXivQuery('./chemrxiv_2020-11-10.jsonl')\nquerier.search_keywords(query, output_filepath='buchwald-hartwig_coupling_ChemRxiv.jsonl')\n\n# Save PDFs in current folder and name the files by their DOI\nsave_pdf_from_dump('./buchwald-hartwig_coupling_ChemRxiv.jsonl', pdf_path='./PDFs', key_to_save='doi')\n\nFor further steps in the data extraction process annotated data in needed to evaluate the extraction pipeline. For this, one could use an annotation tool like doccano which is shown in the following example.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>1.2 | Mining data from ChemRxiv</span>"
    ]
  },
  {
    "objectID": "constrained_decoding/index.html",
    "href": "constrained_decoding/index.html",
    "title": "Constrained generation to guarantee syntactic correctness",
    "section": "",
    "text": "Defining a data schema\nFor most constrained generation tasks, we need to define a data schema in a programmatic way. The most common way to do so is to use pydantic data classes. Here is an example of a simple data schema for a recipe:\nThis schema can also be extended to include descriptions of different fields or to only allow certain values for specific fields. For example, we could add a field for the number of servings and only allow positive integers.\nIf we want to extract copolymerization reactions a data schema could look like the following.\nWe can now use instructor to “patch” the OpenAI API client to ensure that our output fulfils the schema.\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\n\nclass Monomer(BaseModel):\n    name: str = Field(..., title=\"Name\", description=\"Name of the monomer.\")\n    reactivity_constant: Optional[float] = Field(\n        None,\n        title=\"Reactivity constant\",\n        description=\"Reactivity constant of the monomer. r1 for monomer 1 and r2 for monomer 2. Must be greater or equal 0.\",\n        ge=0,\n    )\n    reactivity_constant_error: Optional[float] = Field(\n        None,\n        title=\"Reactivity constant error\",\n        description=\"Error in the reactivity constant. Often indicated with +/-. Must be greater or equal 0\",\n        ge=0,\n    )\n    q_parameter: Optional[float] = Field(\n        None,\n        title=\"Q parameter\",\n        description=\"Q parameter of the monomer. Q1 for monomer 1 and Q2 for monomer 2. Must be greater or equal 0\",\n        ge=0,\n    )\n    e_parameter: Optional[float] = Field(\n        None,\n        title=\"e parameter\",\n        description=\"e parameter of the monomer. e1 for monomer 1 and e2 for monomer 2.\",\n    )\n\n\nclass CopolymerizationReaction(BaseModel):\n    temperature: Optional[float] = Field(\n        ...,\n        title=\"Temperature\",\n        description=\"Temperature at which the reaction is carried out\",\n    )\n    temperature_unit: Optional[Literal[\"C\", \"K\"]] = Field(\n        ..., title=\"Temperature unit\", description=\"Unit of temperature\"\n    )\n    solvent: Optional[str] = Field(\n        None,\n        title=\"Solvent\",\n        description=\"Solvent used in the reaction. If bulk polymerization was performed, this field should be left empty\",\n    )\n    initiator: Optional[str] = Field(\n        None, title=\"Initiator\", description=\"Initiator used in the reaction\"\n    )\n    monomers: Optional[List[Monomer]] = Field(\n        ...,\n        title=\"Monomers\",\n        description=\"Monomers used in the reaction. Ensure that the reactivity ratios are not confused with other numbers (such as Q and e). The two monomers MUST be used in the same reaction and mentioned in the same context.\",\n        min_items=2,\n        max_items=2,\n    )\n    polymerization_type: Optional[str] = Field(\n        ...,\n        title=\"Polymerization type\",\n        description=\"Type of polymerization (e.g., bulk, solution, suspension, emulsion)\",\n    )\n    determination_method: Optional[str] = Field(\n        ...,\n        title=\"Determination method\",\n        description=\"Method used to determine the reactivity ratios (e.g. Kelen Tudor, Fineman-Ross, Mayo-Lewis).\",\n    )\ndiagram = erd.create(CopolymerizationReaction)\ndiagram.draw(\"diagram.svg\")\nSVG(\"diagram.svg\")\nIn this case, we will use PDF files in the form as images as input for the model. To perform this conversion, we import some utilities.\nfrom pdf2image import convert_from_path\nfrom utils import process_image, get_prompt_vision_model\nThe code below only converts each page of the PDF into an image and then generates dictionary objects in a format that can be used by the OpenAI API.\nfilepath = 'paper01.pdf'\npdf_images = convert_from_path(filepath)\n\nimages_base64 = [process_image(image, 2048, 'images', filepath, j)[0] for j, image in enumerate(pdf_images)]\nimages = get_prompt_vision_model(images_base64=images_base64)\nArmed with the images, we can now use the OpenAI API to extract the text from the images. For this, we just call the API with our prompts and the images.\ncompletion = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    response_model=List[CopolymerizationReaction],\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are a scientific assistant, extracting accurate information about co-polymerization reactions from scientific papers.\nDo not use data that was reproduced from other sources.\nIf you confuse the reactivity ratios with other numbers, you will be penalized.\nMonomer names might be quite similar, if you confuse them, you will be penalized.\nNEVER combine data from different reactions, otherwise you will be penalized.\nIf you are unsure, return no data. Quality is more important than quantity.\n\"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"Extract the data from the paper into the provided data schema. We want an iterable of reaction objects and each reaction will be its own object. You can find each page of the paper as an image below.\nThe relationship between monomers and parameters is typically indicated by subscripts that can be a number or an abbreviation of the monomer.\nNever return data that you are not absolutely sure about! You will be penalized for incorrect data.\"\"\",\n        },\n        {\"role\": \"user\", \"content\": [*images]},\n    ],\n    temperature=0,\n)\ncompletion\n\n[CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='carbon tetrachloride', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.54, reactivity_constant_error=0.01, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.06, reactivity_constant_error=0.03, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='chloroform', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.51, reactivity_constant_error=0.01, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.08, reactivity_constant_error=0.03, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='acetone', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.43, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.65, reactivity_constant_error=0.02, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='1,4-dioxane', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.41, reactivity_constant_error=0.02, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.59, reactivity_constant_error=0.05, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos'),\n CopolymerizationReaction(temperature=60.0, temperature_unit='C', solvent='acetonitrile', initiator='AIBN', monomers=[Monomer(name='methacrylic acid', reactivity_constant=0.12, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None), Monomer(name='styrene', reactivity_constant=0.29, reactivity_constant_error=0.0, q_parameter=None, e_parameter=None)], polymerization_type='solution', determination_method='Kelen-Tudos')]",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Constrained generation to guarantee syntactic correctness</span>"
    ]
  },
  {
    "objectID": "constrained_decoding/index.html#defining-a-data-schema",
    "href": "constrained_decoding/index.html#defining-a-data-schema",
    "title": "Constrained generation to guarantee syntactic correctness",
    "section": "",
    "text": "from pydantic import BaseModel\n\nclass Recipe(BaseModel):\n    title: str\n    ingredients: List[str]\n    instructions: List[str]\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, List\n\nclass Recipe(BaseModel):\n    title: str\n    ingredients: List[str]\n    instructions: List[str]\n    servings: int = Field(..., gt=0, description=\"The number of servings for this recipe\")\n    rating: Literal[\"easy\", \"medium\", \"hard\"] = Field(\"easy\", description=\"The difficulty level of this recipe\")",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Constrained generation to guarantee syntactic correctness</span>"
    ]
  },
  {
    "objectID": "agents/agent.html",
    "href": "agents/agent.html",
    "title": "5 | Agents",
    "section": "",
    "text": "References",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>5 | Agents</span>"
    ]
  },
  {
    "objectID": "ocr/nougat/nougat.html",
    "href": "ocr/nougat/nougat.html",
    "title": "OCR with Nougat",
    "section": "",
    "text": "Cleaning the data\nFor certain applications it might be necessary to clean the data before using it for other downstream tasks.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>OCR with Nougat</span>"
    ]
  },
  {
    "objectID": "agents/agent.html#references",
    "href": "agents/agent.html#references",
    "title": "5 | Agents",
    "section": "",
    "text": "Ogasawara, Masamichi, Shiro Wada, Erika Isshiki, Takumi Kamimura, Akira Yanagisawa, Tamotsu Takahashi, and Kazuhiro Yoshida. 2015. “Enantioselective Synthesis of Planar-Chiral Ferrocene-Fused 4-Pyridones and Their Application in Construction of Pyridine-Based Organocatalyst Library.” Organic Letters 17 (9): 2286–89. https://doi.org/10.1021/acs.orglett.5b01044.\n\n\nQian, Yujie, Jiang Guo, Zhengkai Tu, Connor W. Coley, and Regina Barzilay. n.d. “RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing.” Journal of Chemical Information and Modeling. https://doi.org/10.1021/acs.jcim.3c00439.",
    "crumbs": [
      "Data extraction workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>5 | Agents</span>"
    ]
  }
]